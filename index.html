<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>林场的梦</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="林场的梦">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="林场的梦">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="林场的梦">
  
    <link rel="alternate" href="/atom.xml" title="林场的梦" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">林场的梦</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-用户行为的深度追踪——事件与埋点" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/27/用户行为的深度追踪——事件与埋点/" class="article-date">
  <time datetime="2019-06-27T01:03:00.000Z" itemprop="datePublished">2019-06-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/27/用户行为的深度追踪——事件与埋点/">用户行为的深度追踪——事件与埋点</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/27/用户行为的深度追踪——事件与埋点/" data-id="cjxdyu82j00082owfdj30e3oc" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/big-data/">big data</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-糖豆推荐系统第一期开发与评估报告" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/24/糖豆推荐系统第一期开发与评估报告/" class="article-date">
  <time datetime="2019-06-24T14:43:03.000Z" itemprop="datePublished">2019-06-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/24/糖豆推荐系统第一期开发与评估报告/">糖豆推荐系统第一期开发与评估报告</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-缘起"><a href="#1-缘起" class="headerlink" title="1.缘起"></a>1.缘起</h2><p>糖豆作为国内最大的广场舞平台，全网的MAU已经超过4000万，每月PGC和UCG生产的视频个数已经超过15万个，每月用户观看的视频也超过100万个。然而之前糖豆APP首页主要还是依赖内容编辑手工推荐来发现内容，每天的推荐量也是几十个而已。明显可见千人一面的内容分发效率比较低下，继而我们于2016年12月初，启动了糖豆推荐系统的设计以及开发，目前截止到2017年1月初，已经完成第一期推荐系统的开发与评估。推荐项目立项伊始，我撰写了一篇整体架构与设计，本文和架构一文在部分内容有所重复，本文主要专注阐述推荐系统的开发、实现以及评估的细节。</p>
<p>推荐系统的目的也可以简单总结成为以下两点：</p>
<ul>
<li>根据用户个人兴趣分发内容，为生产者和消费者打造更加合理的流量分发体系。</li>
<li>提高用户观看时长，从而进一步到达提升产品留存。</li>
</ul>
<p>可以看到核心评估目标是用户的观看时长，相对直接易理解。当然评估过程，我们遵循数据科学的评估体系，衡量了包括多种优化目标(RMSE,P@K,AUC/ROC,覆盖率等等)的指标。同时还根据AB测试，评估了整体推荐模块的CTR,播放时长等多项业务统计指标。</p>
<h2 id="2-架构"><a href="#2-架构" class="headerlink" title="2.架构"></a>2.架构</h2><p>相信自从Netfix公布他们的推荐架构之后[1]，后续的推荐系统基本都会按照在线(online),近线(near line),离线(off line)三个部分来构建。虽然划分成三个模块，本质是推荐算法迭代时间窗口问题，根据用户行为数据，构建一个持续进化的系统。</p>
<p>糖豆推荐系统架构基本也是按照三个模块来构建。限于人力和时间，第一期主要实现了离线部分。架构图如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-44cc02855ae14481.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="推荐系统架构1.0"></p>
<p>整个系统架构主要由数据、算法、策略、评估和服务层组成，相对清晰明了。</p>
<ol>
<li>数据层，主要数据来源包括用户行为日志以及数据库。我们在16年10月份~11月份，对整个日志收容、分析和挖掘流程做了改造。<ul>
<li>收容：同时将日志离线和在线的pipeline彻底分离。</li>
<li>解析：原本基于MR的ETL全部改为Spark任务，在集群机器数量不变情况下，整体效率基本提高了两倍以上，Spark具备很好的取代MR的潜力。</li>
<li>挖掘：Spark MLib集成了多种机器学习算法，原有基于Mahout的算法基本可以替代实现。</li>
</ul>
</li>
<li>算法层架构一图中，黑字部分是我们实现了的算法，蓝字部分都是计划中但未实现的算法。</li>
<li>策略层：<ul>
<li>融合算法，主要包括以下三种，目前我们同时使用了级联联合以及混合融合。</li>
<li>业务过滤，目前暂时没做。</li>
<li>推荐排序，目前的排序对用户隐式反馈行为(包括播放时长、下载、收藏等指标)做线性加权以及归一化处理，得到一个0~5分之间的评分，作为LFM的数据集，通过模型得到预测的打分，最后按照视频打分以及视频创建时间做倒序排序。后续我们会引入学习排序(LTR)算法，来持续改进推荐结果排序质量。LTR，包括PointWise,PairWise,ListWise三类算法。预期未来先使用PointWise类别的算法。</li>
</ul>
</li>
</ol>
<h2 id="3-算法实现"><a href="#3-算法实现" class="headerlink" title="3.算法实现"></a>3.算法实现</h2><p>推荐系统算法在过去几十年有非常长足的发展和应用，总结下来基本包括基于内容、基于邻域，基于矩阵分解等类型。</p>
<ol>
<li>基于邻域：核心思想是，为用户推荐与之属性、行为相似的物品。邻域就是兴趣相似的数学表达。它包括UserCF和ItemCF，基础研究深入，在性能、可解释性上效果都不错，所以应用也十分广泛。</li>
<li>基于矩阵分解：也就是隐语义模型，在文本挖掘范围首先被提出。矩阵分解是一系列复杂算法(LSM,LSI,LDA,Topic Model)的数学基础。它包括特征值分解、奇异值分解等，有具体计算方法包括SVD,Funk-SVD,ALS,SVD++等。</li>
</ol>
<h3 id="3-1-LFM"><a href="#3-1-LFM" class="headerlink" title="3.1 LFM"></a>3.1 LFM</h3><p> 隐语义模型其核心思想是通过潜在特征联系用户和物品,根据用户行为统计的自动聚类。LFM模型能够划分出多维度、软性、不同权重的分类。它通过以下数学公式来表达用户对物品的兴趣，由两个低秩的矩阵来近似表达原有高阶矩阵。</p>
<p> <img src="http://upload-images.jianshu.io/upload_images/20467-f7af8324276d0a17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="矩阵分解"></p>
<p>可以看到从矩阵计算问题，转化成优化问题。优化目标的数学形式化：</p>
<p> <img src="http://upload-images.jianshu.io/upload_images/20467-b4898e9bb730b901.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="优化形式化"></p>
<p>这个形式化问题有多种解法，包括SVD,ALS等。Spark提供了包括mlib里的ALS，以及graphx里的SVD++。</p>
<h4 id="3-1-1-ALS-最小交替二乘法"><a href="#3-1-1-ALS-最小交替二乘法" class="headerlink" title="3.1.1 ALS(最小交替二乘法)"></a>3.1.1 ALS(最小交替二乘法)</h4><p>ALS将矩阵计算转化成为一个最优化函数问题,通过最小化误差的平方和计算最佳函数匹配。ALS在每次迭代期间，一个因子矩阵保持恒定，而另一个使用最小二乘法求解。同样在求解另一因子矩阵，保持新求解的因子矩阵固定不变。</p>
<p>Spark ALS的实现，每次迭代过程了为了减少通讯消耗，只会传输两个因子矩阵(用户、物品)之一参与计算。这个实现是通过预计算矩阵的元数据，得到一个meta矩阵。这样就可以在用户和物品block之间只传输一组特征向量，来更新计算。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-a96a4fffa564ed41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="ALS"></p>
<ul>
<li>优点，不受到用户和数据质量影响。全局性求解，单一模型效果最好。</li>
<li>缺点，增量更新缓慢。</li>
</ul>
<h4 id="3-1-2-Spark实现"><a href="#3-1-2-Spark实现" class="headerlink" title="3.1.2 Spark实现"></a>3.1.2 Spark实现</h4><p>spark mlib实现了ALS算法，调用比较简单，稍微麻烦的是调参和评估。贴段python代码，注释比较详细了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##初始化sparksession(spark 2.0以上引入)</span></span><br><span class="line">spark = SparkSession.builder.master(<span class="string">'yarn-client'</span>).appName(<span class="string">'recy_als_model:'</span>+inUVMDate).config(<span class="string">'spark.sql.warehouse.dir'</span>, <span class="string">'/user/hive/warehouse'</span>).enableHiveSupport().getOrCreate()</span><br><span class="line"><span class="comment">#读入用户视频评分全量表</span></span><br><span class="line">rateSql =  <span class="string">"select * from da.recy_als_data_uvm where dt='"</span>+inUVMDate+<span class="string">"'"</span></span><br><span class="line"><span class="comment">#spark 读hive表</span></span><br><span class="line">rating = spark.sql(rateSql)</span><br><span class="line"><span class="comment">#分割训练集和测试集,0.8,0.2</span></span><br><span class="line">(training, test) = rating.randomSplit([<span class="number">0.8</span>, <span class="number">0.2</span>])</span><br><span class="line"><span class="comment">#ALS模型参数</span></span><br><span class="line">ranks = [<span class="number">8</span>, <span class="number">10</span>]</span><br><span class="line">lambdas = [<span class="number">0.01</span>,<span class="number">0.05</span>, <span class="number">0.1</span>]</span><br><span class="line">numIters = [<span class="number">20</span>]</span><br><span class="line">bestModel = <span class="literal">None</span></span><br><span class="line">bestValidationRmse = float(<span class="string">"inf"</span>)</span><br><span class="line">bestRank = <span class="number">0</span></span><br><span class="line">bestLambda = <span class="number">-1.0</span></span><br><span class="line">bestNumIter = <span class="number">-1</span></span><br><span class="line"><span class="comment">#调参</span></span><br><span class="line"><span class="keyword">for</span> rank, lmbda, numIter <span class="keyword">in</span> itertools.product(ranks, lambdas, numIters):</span><br><span class="line">    als = ALS(rank=rank,maxIter=numIter, regParam=lmbda, userCol=<span class="string">"f_diu"</span>, itemCol=<span class="string">"f_vid"</span>, ratingCol=<span class="string">"f_rating"</span>, nonnegative=<span class="literal">True</span>)</span><br><span class="line">    model = als.fit(training)</span><br><span class="line">    <span class="comment">#!!注意是随机取样，使用测试集评估模型，通过RMSE来评估模型。由于测试集中可能有模型中没出现过的user,那就会有预测值为nan。drop即可</span></span><br><span class="line">    predictions = model.transform(test).dropna(<span class="string">'any'</span>)</span><br><span class="line">    evaluator = RegressionEvaluator(metricName=<span class="string">"rmse"</span>, labelCol=<span class="string">"f_rating"</span>,</span><br><span class="line">                                    predictionCol=<span class="string">"prediction"</span>)</span><br><span class="line">    validationRmse = evaluator.evaluate(predictions)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"RMSE (validation) = %f for the model trained with "</span> % validationRmse + \</span><br><span class="line">          <span class="string">"rank = %d, lambda = %.1f, and numIter = %d."</span> % (rank, lmbda, numIter)</span><br><span class="line">    <span class="keyword">if</span> (validationRmse &lt; bestValidationRmse):</span><br><span class="line">        bestModel = model</span><br><span class="line">        bestValidationRmse = validationRmse</span><br><span class="line">        bestRank = rank</span><br><span class="line">        bestLambda = lmbda</span><br><span class="line">        bestNumIter = numIter</span><br><span class="line"><span class="comment"># evaluate the best model on the test set</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"The best model was trained with rank = %d and lambda = %.1f, "</span> % (bestRank, bestLambda) \</span><br><span class="line">  + <span class="string">"and numIter = %d, and its RMSE on the test set is %f."</span> % (bestNumIter, bestValidationRmse)</span><br><span class="line"><span class="comment">#保存预测结果</span></span><br><span class="line">predictions = bestModel.transform(rating).dropna(<span class="string">'any'</span>)</span><br><span class="line">predictPath =  <span class="string">"hdfs://Ucluster/olap/da/recy_als_predict/"</span>+inUVMDate+<span class="string">"/"</span></span><br><span class="line">predictions.repartition(<span class="number">200</span>).write.mode(<span class="string">'overwrite'</span>).save(predictPath, format=<span class="string">"parquet"</span>)</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<p>spark ml库在逐步取代mlib库，我们使用了ml，上面代码片段需要引入pyspark.ml相关的类。</p>
<h4 id="3-1-3-候选集问题"><a href="#3-1-3-候选集问题" class="headerlink" title="3.1.3 候选集问题"></a>3.1.3 候选集问题</h4><p>我们训练模型数据量基本在10亿量级，我们计算集群总共16台8核，24G的datanode，训练时间大概30分钟。按照我们用户和物品规模，如果直接使用模型预测推荐结果，候选集规模在<strong>万亿</strong>级别，是集群无法承受的。所有需要对预测的候选集做过滤，目前采用三种过滤方法。</p>
<ol>
<li>看过的作者。将用户过去30天看过的作者的作品作为候选集。这个做法合理清晰，但是存在所谓的“信息茧房”问题，也就是说容易出现多样性不足。</li>
<li>看过的相似的视频。根据ItemCF算法得到相似的视频。将过去看过30天的Top10的类似视频当作候选集。</li>
<li>看过的相似的标签的视频。将用户看过的视频相同类型标签的视频作为候选集。依赖专家知识，在具体到我们的舞蹈视频上，我们编辑提供的标签只能覆盖极少的视频。由于这种做法倾向于PGC作者，在测试后期不再使用。</li>
</ol>
<h3 id="3-2-ItemCF-基于物品的协同过滤"><a href="#3-2-ItemCF-基于物品的协同过滤" class="headerlink" title="3.2 ItemCF(基于物品的协同过滤)"></a>3.2 ItemCF(基于物品的协同过滤)</h3><p>基于物品的协同过滤算法是目前应用最广泛的推荐算法，由亚马逊提出[2]，核心思想给用户推荐那些和他们之前喜欢物品相似的物品。相似度是基于用户对物品的行为来计算的，而非物品本身的属性。</p>
<h4 id="3-2-1-算法原理"><a href="#3-2-1-算法原理" class="headerlink" title="3.2.1 算法原理"></a>3.2.1 算法原理</h4><p>基于物品的协同过滤算法主要分为以下两步：</p>
<ol>
<li>计算物品之间的相似度</li>
<li>根据物品的相似度和用户历史行为给用户生成推荐列表</li>
</ol>
<p><strong>核心是计算物品之间的相似度，我们使用余弦相似度。</strong></p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-119439396cd1167c.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="余弦相似度"></p>
<p>该算法惩罚了热门物品的权重，减轻热门视频和大量视频相似的可能性。</p>
<h4 id="3-2-2-Spark实现"><a href="#3-2-2-Spark实现" class="headerlink" title="3.2.2 Spark实现"></a>3.2.2 Spark实现</h4><p>我们基于spark sql实现了ItemCF，贴一段</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">spark = SparkSession.builder.master(<span class="string">'yarn-client'</span>).appName(<span class="string">'recy_icf_similarity:'</span>+inDate).config(<span class="string">'spark.sql.warehouse.dir'</span>, <span class="string">'/user/hive/warehouse'</span>).enableHiveSupport().getOrCreate()</span><br><span class="line"><span class="comment">#指定spark 分区数</span></span><br><span class="line">spark.sql(<span class="string">"SET spark.sql.shuffle.partitions=2000"</span>)</span><br><span class="line">spark.sql(<span class="string">"drop table if exists da.recy_icf_similarity_mid "</span>)</span><br><span class="line">spark.sql(<span class="string">"create table da.recy_icf_similarity_mid as select a.vid vid_1 , b.vid vid_2 , a.num num_1, b.num num_2, count(1) num_12 from da.recy_icf_similarity_pre a join da.recy_icf_similarity_pre b on (a.diu=b.diu) where a.vid&lt;b.vid group by a.vid, b.vid, a.num, b.num"</span>)</span><br><span class="line"><span class="comment">#计算余弦相似度</span></span><br><span class="line">similarSql = <span class="string">" select vid_1, vid_2, num_12/sqrt(num_1*num_2) similarity from da.recy_icf_similarity_mid"</span></span><br><span class="line">similarDF = spark.sql(similarSql)</span><br><span class="line">similarDF.printSchema()</span><br><span class="line"><span class="comment"># 保存结果</span></span><br><span class="line">similarDF.repartition(<span class="number">300</span>).write.mode(<span class="string">'overwrite'</span>).save(similarDir, format=<span class="string">"parquet"</span>)</span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<h3 id="3-3-抄底策略"><a href="#3-3-抄底策略" class="headerlink" title="3.3 抄底策略"></a>3.3 抄底策略</h3><p>抄底策略其实是一个冷启动的问题，策略也非常多。</p>
<ul>
<li>近期热门item、新item。</li>
<li>编辑精选。</li>
<li>新品类上线。</li>
<li>同城热门。</li>
</ul>
<p>我们目前只生效了热门策略，采用了Hack News的热门算法作为抄底策略，如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-8f24c3e4a80d5848.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="热门算法"></p>
<ul>
<li>P表示视频观看次数。</li>
<li>T表示距离视频发布时间(单位为小时),加上2是为了防止最新的视频导致分母过小。</li>
<li>G表示”重力因子”(gravityth power),即为视频衰减系数。</li>
</ul>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-73cebaf5f18169b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="热门算法衰减系数"></p>
<p>我们根据实验结果，确定了G的取值。该算法同时保证了视频的热门程度和新鲜度。sql代码如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> vid,title,createtime,hits_total,(<span class="keyword">if</span>( hits_total&gt;=<span class="number">1</span>, hits_total - <span class="number">1</span>,hits_total)/<span class="keyword">power</span>((<span class="keyword">TIMESTAMPDIFF</span>(<span class="keyword">hour</span>,createtime,<span class="keyword">now</span>())+<span class="number">2</span>),<span class="number">1.8</span>)) <span class="keyword">as</span> sc <span class="keyword">FROM</span> <span class="string">`video`</span> <span class="keyword">WHERE</span> <span class="built_in">date</span>(createtime) &gt;=<span class="keyword">NOW</span>() - <span class="built_in">INTERVAL</span> <span class="number">3</span> <span class="keyword">DAY</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="string">`sc`</span></span><br></pre></td></tr></table></figure>

<h3 id="3-4-算法融合"><a href="#3-4-算法融合" class="headerlink" title="3.4 算法融合"></a>3.4 算法融合</h3><p>融合策略主要包括以下三类，当然还有ensemble相关的方法：</p>
<ul>
<li>加权融合(Weight Merge):根据经验值对不同算法赋给不同的权重，对各个算法产生的候选集按照给定的权重进行加权，然后再按照权重排序</li>
<li>级联融合(Cascade Merge): 优先采用效果好的算法，当产生的候选集大小不足以满足目标值时，再使用效果次好的算法。</li>
<li>混合融合(Mix Merge): 不同的算法按照不同的比例产生一定量的候选集，然后叠加产生最终总的候选集。</li>
</ul>
<p>我们主要在候选集上使用了mix merge，在结果产出时，采用了cascade merge合并LFM和ItemCF的结果。</p>
<h2 id="4-服务实现"><a href="#4-服务实现" class="headerlink" title="4.服务实现"></a>4.服务实现</h2><h3 id="4-1-AB分桶服务"><a href="#4-1-AB分桶服务" class="headerlink" title="4.1 AB分桶服务"></a>4.1 AB分桶服务</h3><p>根据用户diu，使用crc32 hash函数对用户取余，分别赋予AB两个类型。客户端拿到abtag后根据服务端数据流实现展示和数据埋点。</p>
<h3 id="4-2-推荐服务"><a href="#4-2-推荐服务" class="headerlink" title="4.2 推荐服务"></a>4.2 推荐服务</h3><p>个性化推荐系统服务会在app首页打开后被调用，具体服务流程步骤如下：</p>
<ol>
<li>通过用户DIU获取推荐模型导出的数据列表</li>
<li>判断推荐的数据列表是否为空</li>
<li>推荐的数据列表如果不为空，则执行5</li>
<li>推荐的数据列表如果为空，则获取抄底的推荐列表，然后执行5</li>
<li>从推荐的数据列表中过滤点目前首页已经展现的视频</li>
<li>根据推荐的分数和视频创建时间，将列表进行排序</li>
<li>返回结果</li>
</ol>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-c5d4f88b26c7a72d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="流程图"></p>
<h3 id="4-3-存储选型"><a href="#4-3-存储选型" class="headerlink" title="4.3 存储选型"></a>4.3 存储选型</h3><p>推荐系统每天出一次推荐结果， 因此<code>推荐结果需要按天区分</code>, 同时需要按diu来快速查询，可以采用的存储有<code>hbase</code>，<code>redis</code>等键值对数据库，<code>mongodb</code>等文档型数据库，或者<br><code>mysql</code>等传统关系型数据库</p>
<ul>
<li><strong>hbase</strong> 键值对存储，存储量大，查询速度快，稳定性取决于集群是否高可用，如高可用，可优先选择</li>
<li><strong>redis</strong> 键值对存储，存储量较大，热数据基于内存存储，查询速度快，可以考虑，不过当每个人的推荐结果N较大时，要考虑存储大小</li>
<li><strong>mongodb</strong> 文档型数据库，存储量大，热数据同样存储在内存，索引速度接近于redis， 结构化，易维护，可以考虑</li>
<li><strong>mysql</strong> 关系型数据库， 存储量较大，基于文件索引机制，查询速度较上述存储来说，理论值较低，可以作为备选。</li>
</ul>
<p>每个用户的推荐数N=60， 存储占用<code>180g</code>，决定采用hbase 根据rowkey字段做索引， 当我们指定<code>diu</code>和<code>date</code>时，会快速返回rowkey在该范围内的结果。</p>
<h2 id="5-效果评估"><a href="#5-效果评估" class="headerlink" title="5.效果评估"></a>5.效果评估</h2><h3 id="5-1-离线评估"><a href="#5-1-离线评估" class="headerlink" title="5.1 离线评估"></a>5.1 离线评估</h3><p>采用融合多维度用户行为数据线性转换成显式反馈评分。由于采用了多维度数据，算法模型效果大幅提升，结果如下：</p>
<ul>
<li>RMSE从4.1提升到1.0。(Netfix大赛冠军大概在0.8左右)</li>
<li>P@K从0.6705提升到0.938。</li>
<li>预测覆盖率为99%,推荐覆盖率为90%。</li>
</ul>
<h3 id="5-2-A-B测试"><a href="#5-2-A-B测试" class="headerlink" title="5.2 A/B测试"></a>5.2 A/B测试</h3><p>猜你喜欢模块已经在官方渠道测试将近三周，展现形式如下图：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-134910bd586cd114.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="猜你喜欢"></p>
<p>通过AB测试，可以看到首页模块的点击率整体提升了10%，人均观看时长整体提升5%。目前可以看到，猜你喜欢模块效果略优于每日精选。</p>
<h2 id="6-改进与展望"><a href="#6-改进与展望" class="headerlink" title="6.改进与展望"></a>6.改进与展望</h2><p>第一期开发的时间相对较短，人力也非常不足，期间还有很多数据分析、挖掘工作需要兼顾，整体工作相对简单。未来第二期，主要精力集中在近线和在线的模块开发，以及学习排序。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/24/糖豆推荐系统第一期开发与评估报告/" data-id="cjxdyu82n000d2owfm31402nb" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/recommendation-system/">recommendation system</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-糖豆数据仓库模型" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/24/糖豆数据仓库模型/" class="article-date">
  <time datetime="2019-06-24T14:42:26.000Z" itemprop="datePublished">2019-06-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/24/糖豆数据仓库模型/">糖豆数据仓库模型</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>数据仓库是面向主题的、集成的、时变的和非易失的有组织的数据集合，支持管理决策制定。不同于面向OLTP（<em>On-Line Transaction Processing</em>）数据库建设，数据仓库为OLAP（<em>On-Line Analytical Processing</em>）而生。</p>
<p>本文着手规划cover糖豆全业务线的数据仓库建设。先给出糖豆数仓模型，给出糖豆数仓的理论依据；再在此基础上根据糖豆各业务线的实际需求，给出各个业务主题的具体数据集市建设模型。</p>
<h2 id="一、数据仓库的分层结构"><a href="#一、数据仓库的分层结构" class="headerlink" title="一、数据仓库的分层结构"></a>一、数据仓库的分层结构</h2><p>一般情况下，数据仓库往往采用三层结构。底层是数据仓库服务器，在我们这里就是由hive cover的分布式文件存储系统。中间层是OLAP服务器。上层是用户，包括查询和报表工具。</p>
<p>联机分析处理(OLAP)可以在使用多维数据模型的数据仓库或数据集市上进行。典型的 OLAP操作包括<strong>上卷</strong>、<strong>下钻(钻过、钻透)</strong>、<strong>切片</strong>和<strong>切块</strong>、<strong>转轴(旋转)</strong>，以及求等级、计算平均值和增长率等统计操作。使用数据立方体结构，OLAP 操作可以有效地实现。</p>
<p>OLAP服务器可以是关系OLAP(ROLAP)，多维OLAP(MOLAP)，或混合OLAP(HOLAP)。ROLAP服务器使用扩充的关系 DBMS，将多维数据上的 OLAP 操作映射成标准的关系操作。MOLAP 服务器直接将多维数据视图映射到数组结构。HOLAP 是 ROLAP 和 MOLAP 的结合。例如，它可以对历史数据使用 ROLAP，而将频繁访问的数据放在一个分离的 MOLAP 存储中。</p>
<p><img src="http://ol5qowkge.bkt.clouddn.com/dw/dwlayer.bmp" alt></p>
<p>关于这部分内容更多的了解可以参考《数据挖掘概念与技术》中关于数据仓库的介绍。在本规划中着重介绍的是底层——数据仓库服务器层，也就是hive数据仓库的建设。重点涉及<strong>基础层(ODS)、中间层(EDW)、集市层(ADM)</strong>的规划建设。中间层（OLAP服务器层）与顶层（前端可视化层），暂不考虑，将来可能会考虑采用业界开源/免费的集成解决方案，如airbnb开源的<a href="https://github.com/airbnb/superset" target="_blank" rel="noopener">Superset</a> (之前叫做Caravel / Panoramix)、一直免费的<a href="http://meteorite.bi/products/saiku" target="_blank" rel="noopener">saiku</a>。</p>
<p>hive数据仓库部分，我们将按传统数据仓库分层思想，分为三层：</p>
<ul>
<li>ODS(Operation Data Storage)层——操作数据存储层，在这里就是<strong>原始数据层</strong>。主要包括三部分数据，从业务服务器实时采集的业务原始日志、对业务原始日志解析后结构化的日志宽表、通过sqoop从业务数据库导入的业务数据。这层数据处理原则是不丢失，尽量保证数据的原貌。除了不能解析的异常日志，尽量保证日志条数不丢失；日志字段不清洗，不care日志字段的具体取值是否合法，只负责字段的结构化存储。这部分数据需要全量压缩保存。这层存储的是最细粒度的数据。</li>
<li>EDW(Enterprise Data Warehouse)层——企业级数据仓库层，在这里就是<strong>中间基础数据层</strong>。EDW在传统行业一般是指整个数仓，甚至包括ODS层；在大型的集团公司，EDW一般指各独立业务线的数仓，如财务数仓，交易数仓，日志数仓。在我们这里，EDW主要是日志数仓明细，也有称作DWB(DataWare Base)层的。中间基础数据层的数据应该是<strong>一致的、准确的、干净的</strong>，它对原始数据层进行了清洗转换。这层存储的也是最细粒度的数据，和ODS层相同。</li>
<li>ADM(Aggregative Data Model)层——聚合数据层，在这里就是<strong>数据集市层</strong>。数据集市层对EDW层的数据做不同程度的聚合汇总，已经不存在明显数据。这层数据从纵向上讲是面向业务主题来组织的，用来支撑多维分析，我们采用星形结构。这层中每张事实表中的一行都是一条用户操作记录的聚合，比如用户在某个模块观看某个视频的总时长。</li>
</ul>
<h2 id="二、糖豆数仓业务模型"><a href="#二、糖豆数仓业务模型" class="headerlink" title="二、糖豆数仓业务模型"></a>二、糖豆数仓业务模型</h2><h4 id="1、业务线梳理"><a href="#1、业务线梳理" class="headerlink" title="1、业务线梳理"></a>1、业务线梳理</h4><p>按目前发展状况，根据职能梳理的业务线如下图所示。其中大部分业务线需要根据具体业务需求规划数据集市层建设。优先挑选重要紧急的业务线做。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-2111a10f3c8dcf0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p>
<h4 id="2、业务主题建模"><a href="#2、业务主题建模" class="headerlink" title="2、业务主题建模"></a>2、业务主题建模</h4><p>以推荐业务为主线举例：</p>
<p>推荐目前的产品形态为【猜你喜欢】（包括离线/准实时/实时）与【相关推荐】。推荐业务目前最重要的数据需求之一就是效果对比评估。</p>
<p>所以对于推荐业务主题域，主要的事实表就是用户在推荐产品模块内的操作数据，包括用户的浏览，点击播放，后续的关注、评论等。</p>
<p>在业务建模阶段，我们倾向于使用<strong>实体建模法</strong>，实体建模可以很轻松的完成对现实世界的抽象，把整个业务划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。实体建模的具体介绍可以参考链接1。在实体建模中，任何一个业务都可以划分为3个部分，实体，事件和说明：</p>
<ul>
<li><p>实体，主要指领域模型中特定的概念主体，指发生业务关系的对象。</p>
</li>
<li><p>事件，主要指概念主体之间完成一次业务流程的过程，特指特定的业务过程。</p>
</li>
<li><p>说明，主要是针对实体和事件的特殊说明。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-f057c88bf8b765ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p>
</li>
</ul>
<p>如图所示，我们描述了一个简单的事实：用户在推荐模块A，点击了算法B推荐的视频。以这个业务事实为例，我们可以把“用户”，“视频 ”看成是一个实体，“点击”描述的是一个业务过程，我们在这里可以抽象为一个具体“事件”，而“在推荐模块A，通过算法B”则可以看成是事件“点击”的一个说明。</p>
<p>该主题域内的实体对象有【视频】、【用户】、【推荐算法】、【推荐模块】等，事件有【浏览/曝光】、【播放/点击】。围绕浏览与点击两个事件，组合视频、用户、算法、模块等实体间的关系，就会发现我们的业务相当简单，可以梳理出以下推荐业务事实：</p>
<ul>
<li>用户在某推荐模块<strong>浏览</strong>某推荐算法推荐的视频<ul>
<li>被某推荐算法推荐的视频在某推荐模块被用户浏览</li>
</ul>
</li>
<li>用户在某推荐模块<strong>播放</strong>某推荐算法推荐的视频<ul>
<li>用户在某推荐模块播放某推荐算法推荐的视频时长</li>
</ul>
</li>
</ul>
<p>基于上面两个基础事实，我们可以再聚合。在推荐评估中，我们非常关注某个算法表现或某个推荐模块的表现。所以，以推荐模块、推荐算法为实体，我们可以梳理出以下事实：</p>
<ul>
<li>推荐模块内各算法推荐的视频的曝光/播放</li>
<li>推荐算法在不同推荐模块的推荐视频的曝光/播放</li>
</ul>
<p>梳理出这些基础业务事实，我们再来看各业务实体的属性/维度。比如，视频这个实体，有时长、up主/作者…；用户实体有是否注册、UGC/PGC、地域等属性。在不同的业务主体域内，同一实体可能有不同的属性/维度，比如用户实体在推荐业务中，我们可能会关注他的画像属性，如兴趣偏好；而在内容编辑业务中，对于用户实体肯定要知道ta是否是up主，是UGC还是PGC。</p>
<p>所以在不同的业务域内，同一实体内的维表可能有不同的字段，这个就是下一步维度建模涉及的问题。维度建模需要手动维护维度表数据的一致性。</p>
<h2 id="三、糖豆数仓维度建模（事实星座模型）"><a href="#三、糖豆数仓维度建模（事实星座模型）" class="headerlink" title="三、糖豆数仓维度建模（事实星座模型）"></a>三、糖豆数仓维度建模（事实星座模型）</h2><p>上面以推荐业务主线为例，我们分析了推荐主题的业务模型。上面的建模分析仍处于数据仓库建模的业务建模或领域建模阶段，该节段仍然是对现实业务的初步抽象分析。数仓建设最重要的步骤是逻辑建模，最终的代码实施则是物理建模阶段。逻辑建模直接决定了物理建模的成败，它决定物理模型实现的难度与是否能满足业务分析需求。</p>
<p>糖豆数仓的逻辑建模我们采用最流行的多维分析模型——星形模型。通常，多维数据模型用于数据仓库和数据集市的设计。这种模型采用星形模式、雪花模式或事实星座模式。多维数据模型的核心是数据立方体。数据立方体由大量事实(或度量)和许多维度组成。维度是一个组织想要记录的实体或透视，是自然分层的。支撑多维分析的关键就在于维度建模。</p>
<p>进行维度建模之前，我们首先了解两个概念：</p>
<p><strong>事实表</strong>——发生在现实世界中的操作型事件，其所产生的可度量数值，存储在事实表中。从最低的粒度级别来看，事实表行对应一个度量事件。在上面的业务建模中，我们已经梳理推荐业务的业务事实，对应的，就是一张张事实表。</p>
<p><strong>维度表/维表</strong>——每个维度表都包含单一的主键列。维度表的主键可以作为与之关联的任何事实表的外键，当然，维度表行的描述环境应与事实表行完全对应。 维度表通常比较宽，是扁平型非规范表，包含大量的低粒度的文本属性。</p>
<p>还以推荐业务为例，我们来分析推荐主题的维度建模。考虑用户在推荐模块点击视频的事实。我们建一个大的，包含大量数据、不含冗余数据的中心表（事实表）——推荐点击表；一组小的附属表（维表），每个维度都有一个单独的维表。假设推荐点击事实表有用户、地域、版本、视频、ABTag、模块、算法等维度，包含点击次数一个度量。为尽量减小事实表的尺寸，事实表中的维度都用标示id。每个维表包含一组属性，这些属性可能有一定程度的冗余。例如，地域中广州和深圳都属于华南地区一线城市，区域和城市级别属性会有冗余。多个事实表可能需要共享维表，如推荐点击事实表和推荐播放事实表就会共享地域、用户、版本等许多维表。这种多个事实表可以共享维表的模式可以看做是星形模式的合集，因此被称作<strong>星系模式</strong>，或者叫做<strong>事实星座模型</strong>。在我们的业务中，多维建模就是根据业务事实，建立这种事实星座模型。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-3f7d6926226035cd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p>
<p>图中示例的推荐点击事实表是只做了轻度聚合的事实表，还包含很多细粒度的数据，如用户diu。在数据集市中，往往根据业务主题纵向发展，做不同层次的聚合。我们对上述事实表，对用户做聚合，就能得到视频被点击人数和次数的二次聚合事实表。对用户浏览视频的事实表做聚合，得到视频被曝光人数和次数的事实表。两者关联就得到视频的点击率。所以，这些<strong>基础的、粒度最细的事实表的建设对于面向主题的数据集市的建设非常重要</strong>。在这些大事实表的基础上，对各种维度做不同粒度的聚合，就构成了立体式有各种粒度数据的数据集市。在聚合的过程总对于次数这种度量，可以做任意维度的组合聚合；而对于人数这种涉及去重的度量，不同维度的聚合只能做独立的去重计算。所以对于去重聚合，要事先考虑好需要用到的维度，再做计算。</p>
<h2 id="四、糖豆数仓规划"><a href="#四、糖豆数仓规划" class="headerlink" title="四、糖豆数仓规划"></a>四、糖豆数仓规划</h2><p>根据目前糖豆业务线，结合目前已有的数据表，糖豆数仓的大体构成如下图。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-9e3c89488ef2067d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>1、<a href="https://www.ibm.com/developerworks/cn/data/library/techarticles/dm-0803zhousb/" target="_blank" rel="noopener">浅谈数据仓库建设中的数据建模方法</a></p>
<p>2、<a href>数据挖掘概念与技术</a></p>
<p>3、<a href="http://www.jianshu.com/p/17baa9f96ca7" target="_blank" rel="noopener">漫谈数据仓库之维度建模</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/24/糖豆数据仓库模型/" data-id="cjxdyu82r000i2owf2fnm5hy6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/big-data/">big data</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-单步强化学习在糖豆推荐系统的应用" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/24/单步强化学习在糖豆推荐系统的应用/" class="article-date">
  <time datetime="2019-06-24T14:41:22.000Z" itemprop="datePublished">2019-06-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/24/单步强化学习在糖豆推荐系统的应用/">单步强化学习在糖豆推荐系统的应用</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="1-强化学习基础"><a href="#1-强化学习基础" class="headerlink" title="1.强化学习基础"></a>1.强化学习基础</h3><h4 id="1-1-强化学习概念"><a href="#1-1-强化学习概念" class="headerlink" title="1.1 强化学习概念"></a>1.1 强化学习概念</h4><p>强化学习通常用马尔科夫决策过程(Markov Desicision Process)来描述：机器(agent)在环境(environment)中，状态空间为S，其中每个状态s∈S是机器所处于的环境的描述；机器所能采取动作(Action)，其空间为A；若机器采取动作a∈A作用于当前状态s，潜在的转移概率p会使得环境当前状态s按某种概率转移到另一状态s’，同时环境会根据潜在的奖赏函数(Reward)给机器反馈一个奖赏。因而，强化学习可以用四元组E=&lt;S,A,P,R&gt; 来表达。其图示如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-79d75cbbd3a9a800.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="强化学习图示"></p>
<p>以下举例说明：<br>不同的state采取不同的action，会有一定概率发生状态转移，最后得到不同的reward。<br><img src="http://upload-images.jianshu.io/upload_images/20467-ede78c3d41a0244f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="MDP"></p>
<p>机器要做的是在环境中不断尝试学习到一个最优的策略π，根据该策略，能知道状态s下需要执行的动作a=π(x)。策略优劣取决于长期执行该策略的累计奖赏，它有多种计算方法，包括T步累计奖赏、γ折扣累计奖赏等。其中γ累计折扣奖赏公式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-da28ab7abf1b9a24.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="γ累计折扣奖赏"></p>
<p>可以看到强化学习与监督学习不同的是，最终奖赏一般会体现在 <strong>多步</strong> 动作之后，从某种意义上来说，可以看作具有“延迟标记信息”的监督学习问题。而强化学习的最简单形态，最大化单步奖赏，对应的正是多臂老虎机理论。</p>
<h4 id="1-2-多臂老虎机-MAB"><a href="#1-2-多臂老虎机-MAB" class="headerlink" title="1.2 多臂老虎机(MAB)"></a>1.2 多臂老虎机(MAB)</h4><blockquote>
<p>一个赌徒，要去摇老虎机，走进赌场一看，一排老虎机，外表一模一样，但是每个老虎机吐钱的概率可不一样，他不知道每个老虎机吐钱的概率分布是什么，那么想最大化收益该怎么整？这就是多臂赌博机问题(Multi-armed bandit problem, K-armed bandit problem, MAB)。<br><img src="http://upload-images.jianshu.io/upload_images/20467-cf6bc08d14f97d4e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="拉加维加斯老虎机"></p>
</blockquote>
<p>如果赌徒知道每个摇臂的期望奖赏，那么他只需要“仅利用(exploitation-only),即只要一直按下最大奖赏的摇臂。如果仅为获知每个摇臂的期望奖赏，则采用“仅探索(exploration-only)”，即轮流按下每个摇臂。事实上，“仅利用”和“仅探索”都难以实现累计奖赏最大化。事实上，“探索”和“利用”是矛盾的，欲使累计奖赏最大化，那就需要折中两者。</p>
<p>####1.3 Bandit算法<br>Bandit算法有非常多种，我们采用累积遗憾（regret）来评估一个算法好坏。<br>MAB的每个臂的收益非0即1，也就是伯努利收益。算法每次选择后，计算和最佳的选择差了多少，然后把差距累加起来就是总的遗憾。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-bbc68ffccf2dadba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="累积regret"></p>
<h4 id="ε-Greedy"><a href="#ε-Greedy" class="headerlink" title="ε-Greedy"></a>ε-Greedy</h4><p>选一个(0,1)之间较小的数ε，每次决策以概率ε去勘探Exploration，1-ε的概率来开发Exploitation，基于选择的item及回报，更新item的回报期望，不断循环下去。</p>
<p>####SoftMax</p>
<p>SoftMax利用softmax函数来确定各item的回报的期望概率排序，进而在选择item时考虑该信息，减少exploration过程中低回报率item的选择机会，同时收敛速度也会较ε-Greedy更快。</p>
<h4 id="UCB"><a href="#UCB" class="headerlink" title="UCB"></a>UCB</h4><p>Upper Confidence Bound，步骤如下： 初始化：先对每一个臂都试一遍； 按照如下公式计算每个臂的分数，然后选择分数最大的臂作为选择：<br><img src="http://upload-images.jianshu.io/upload_images/20467-38a467cd9791bf5a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="item期望"></p>
<p>其中，x_j是item_j的平均回报，n_j是item_j截至当前被选择的次数，n为当前选择所有item的次数。上式反映了，均值越大，标准差越小，被选中的概率会越来越大，起到了exploit的作用；同时哪些被选次数较少的item也会得到试验机会，起到了explore的作用。</p>
<h4 id="LinUCB"><a href="#LinUCB" class="headerlink" title="LinUCB"></a>LinUCB</h4><p>UCB没用充分利用上下文信息Contextual，而LinUCB的基本思想是对每个item的回报估计及其置信区间同时建模，然后每次选择回报的估计值与其标准差的和最大的那个item，因此LinUCB在推荐系统中，能够较好地平衡显示用户已经喜欢的某类文章和对其他没怎么看过的类别的文章，从而引导用户对未知类别的探索。</p>
<h4 id="Thompson-sampling"><a href="#Thompson-sampling" class="headerlink" title="Thompson sampling"></a>Thompson sampling</h4><p>假设每个item有一个产生回报的概率p，我们通过不断试验来估计一个置信度较高的概率p的概率分布。如何估计概率p的概率分布呢？ 假设概率p的概率分布符合beta(wins, lose)分布，它有两个参数: wins, lose， 每个item都维护一个beta分布的参数。每次试验选中一个item，有回报则该item的wins增加1，否则lose增加1。每次选择item的方式是：用每个item现有的beta分布产生一个随机数b，选择所有item产生的随机数中最大的那个item。<br><img src="http://upload-images.jianshu.io/upload_images/20467-4b8fe944f8bd2b33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Thompson sampling"></p>
<h4 id="以上各种算法在不同的性能："><a href="#以上各种算法在不同的性能：" class="headerlink" title="以上各种算法在不同的性能："></a>以上各种算法在不同的性能：</h4><p><img src="http://upload-images.jianshu.io/upload_images/20467-6c6a846e4d01cbbf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="bandit算法对比"></p>
<h3 id="2-多臂老虎机的推荐应用"><a href="#2-多臂老虎机的推荐应用" class="headerlink" title="2. 多臂老虎机的推荐应用"></a>2. 多臂老虎机的推荐应用</h3><h4 id="2-1-冷启动"><a href="#2-1-冷启动" class="headerlink" title="2.1 冷启动"></a>2.1 冷启动</h4><blockquote>
<p>计算机广告和推荐系统中，有很多问题可以抽象为E&amp;E问题：</p>
<ul>
<li>user冷启动：假设一个用户对不同类别的内容感兴趣程度不同，那么我们的推荐系统初次见到这个用户时，怎么快速地知道他对每类内容的感兴趣程度？</li>
<li>item冷启动：假设资源池有若干新item，怎么知道该给每个用户展示哪个，从而获得最大的点击，同时还能保证每个item得到一定的曝光？</li>
</ul>
</blockquote>
<p>这些都是糖豆在实际线上业务遇到的问题，我们采用 Thompson sampling算法来解决推荐过程遇到的E&amp;E问题。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BandItTask</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">editorLiteVideo</span><span class="params">()</span></span>&#123;</span><br><span class="line">			<span class="comment">// 获得beta 分布</span></span><br><span class="line">			Random r = <span class="keyword">new</span> Random();</span><br><span class="line">			Map&lt;String, Double&gt; map = <span class="keyword">new</span> TreeMap&lt;String, Double&gt;();</span><br><span class="line">			<span class="keyword">for</span> (Iterator&lt;String&gt; iterator = videos.keySet().iterator(); iterator.hasNext();) &#123;</span><br><span class="line">				String vid = iterator.next();</span><br><span class="line">				Map&lt;String,String&gt; mab = <span class="keyword">null</span>;</span><br><span class="line">				<span class="keyword">try</span>&#123;</span><br><span class="line">					mab = predis.hgetAll(<span class="string">"mab_"</span>+vid);</span><br><span class="line">				&#125;<span class="keyword">catch</span>(Exception e)&#123;</span><br><span class="line">					mab = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">					logger.error(<span class="string">""</span>,e);</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">double</span> win=<span class="number">1.00</span>, lose = <span class="number">1.00</span>;</span><br><span class="line">				<span class="keyword">if</span> (<span class="keyword">null</span> == mab || mab.isEmpty())&#123;<span class="comment">// 如果还没有lose,win</span></span><br><span class="line">					<span class="keyword">if</span> (<span class="keyword">null</span> == items || !items.contains(vid))&#123; <span class="comment">//并且没有给过初始化的sample值,给个初始化值</span></span><br><span class="line">						win = Convert.toDouble(mab.get(<span class="string">"win"</span>),(<span class="keyword">double</span>)r.nextInt(<span class="number">100</span>));</span><br><span class="line">						lose = Convert.toDouble(mab.get(<span class="string">"lose"</span>),(<span class="keyword">double</span>)r.nextInt(<span class="number">100</span>));</span><br><span class="line">					&#125;</span><br><span class="line">				&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">					win = Convert.toDouble(mab.get(<span class="string">"win"</span>),win);</span><br><span class="line">					lose = Convert.toDouble(mab.get(<span class="string">"lose"</span>),lose);</span><br><span class="line">				&#125;</span><br><span class="line">				BetaDistribution beta = <span class="keyword">new</span> BetaDistribution(win, lose);</span><br><span class="line">				<span class="keyword">double</span> p = beta.sample();</span><br><span class="line">				map.put(vid, p);</span><br><span class="line">				logger.debug(<span class="string">"editorLiteVideo - for sample, vid :"</span>+vid+<span class="string">", mab :"</span>+mab+<span class="string">", win :"</span>+win+<span class="string">", lose :"</span>+lose+<span class="string">", p :"</span>+p);</span><br><span class="line">			&#125;</span><br><span class="line">	</span><br><span class="line">	Logger logger = Logger.getLogger(BandItTask.class);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-2-效果评估"><a href="#2-2-效果评估" class="headerlink" title="2.2 效果评估"></a>2.2 效果评估</h4><p>MAB的应用在糖豆不同的推荐数据集和不同用户群体上多次AB测试结果显示，相较仅探索、加权平均分配、阶梯分配等方法，MAB算法的CTR提升了20%~50%。尤其是item和user都是冷启动的场景，能够带来非常显著的提升。</p>
<h3 id="3-不足与改进"><a href="#3-不足与改进" class="headerlink" title="3. 不足与改进"></a>3. 不足与改进</h3><h4 id="不足："><a href="#不足：" class="headerlink" title="不足："></a>不足：</h4><ul>
<li>目前我们实现的MAB是batch形式，会带来不必要的累积regret。</li>
<li>另外bandit实验数据未能和内容分类结合，形成推荐知识累积闭环。<h4 id="改进："><a href="#改进：" class="headerlink" title="改进："></a>改进：</h4></li>
<li>研究MAB的收敛界，增量更新分布，减少regret</li>
<li>研究对比其他contextual bandit</li>
</ul>
<p>###参考文献<br><a href="http://x-algo.cn/index.php/2016/12/15/ee-problem-and-bandit-algorithm-for-recommender-systems/" target="_blank" rel="noopener">推荐系统的EE问题及Bandit算法</a><br><a href="http://www.xfyun.cn/share/?p=2606" target="_blank" rel="noopener">Bandit算法与推荐系统</a><br><a href="https://zhuanlan.zhihu.com/p/21388070" target="_blank" rel="noopener">专治选择困难症——bandit算法</a>  </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/24/单步强化学习在糖豆推荐系统的应用/" data-id="cjxdyu82h00042owfelu4s314" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/reinforcement-learning/">reinforcement learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-糖豆实时推荐系统设计与实现" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/24/糖豆实时推荐系统设计与实现/" class="article-date">
  <time datetime="2019-06-24T14:26:44.000Z" itemprop="datePublished">2019-06-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/24/糖豆实时推荐系统设计与实现/">糖豆实时推荐系统设计与实现</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-实时推荐系统与相关工作"><a href="#1-实时推荐系统与相关工作" class="headerlink" title="1.实时推荐系统与相关工作"></a>1.实时推荐系统与相关工作</h2><p>###1.1 原因<br>实时计算能够及时捕获用户短时兴趣，同时能够快速反馈分发当前系统的用户兴趣内容。大量实践以及发表的文章都显示了推荐系统实时化，对推荐精准度的提升的有效性和必要性。</p>
<h3 id="1-2-腾讯架构与实现"><a href="#1-2-腾讯架构与实现" class="headerlink" title="1.2 腾讯架构与实现"></a>1.2 腾讯架构与实现</h3><p>实时推荐相关工作非常多，腾讯和北大合作的两篇SIGMOD文章是比较实际和详细的实现，采用的计算框架能够支持大规模数据的实时推荐，以下将会分开简述以下两篇文章。</p>
<p>####2015年<br>Huang发表了基于Storm和KV存储的大规模实时推荐系统 (TencentRec: Real-time Stream Recommendation in Practice)</p>
<ol>
<li>实现了一系列经典推荐算法的实时版本</li>
<li>实现了数种实时算法提高推荐精度</li>
<li>广泛应用于业务有效提高</li>
</ol>
<p>腾讯采用使用storm原因，支持实时数据流式计算，良好的可扩展性、可容错性，采用简单编程模型。<br>文章核心包括实时增量计算的ItemCF，以及用户隐式反馈计算、实时剪枝算法、基于用户画像的数据稀疏性策略。应用在多个业务上都有不同程度的提升，最明显的是腾讯视频的全局表现提升高达30%。</p>
<p>全文核心应该是下图六道公式，阐述腾讯如何具体实现的增量itemcf。</p>
<p>文章中的co-rating,其实就是我们常说的user  bias. 公式3和4解决了用户隐式反馈问题，细节的计算可以参考2016的文章，实际是一个log函数融合了用户的浏览、点击、分享、购买等行为，转化成rating.</p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-b5bee96bdb968c10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="corating.png"></p>
<p>请注意公式4，由于他们定义了corating，实际是将相似度的增量计算从L2范数的计算转化成了L1范数计算.(当Rup取x的时候，y=1/x)。</p>
<p>可扩展的增量计算<br><img src="http://upload-images.jianshu.io/upload_images/20467-b783a98e4a094361.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="itemcf.png"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-c913a1b5bafe6a3b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="initemcf.png"></p>
<p>####2016年<br>腾讯视频的推荐应用(Real-time Video Recommendation Exploration)</p>
<ol>
<li>实时处理、大规模数据下的准确率和可扩展性。</li>
<li>开发了一个基于矩阵分解的大规模在线协同过滤算法，以及一系列的自适应更新策略。</li>
<li>通过增加包括视频类别、时间因素影响、用户画像剪枝以及训练等方法，提高实时TopN推荐的精度。</li>
</ol>
<p>在我们看来，全文核心在于实时计算的数据流转，如下图所示：<br><img src="http://upload-images.jianshu.io/upload_images/20467-a81945bcf6b4f4f5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="tecvideo.png"></p>
<p>基于storm的实时计算topology图:<br><img src="http://upload-images.jianshu.io/upload_images/20467-ffc836b868f9de45.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="topo.png"></p>
<h2 id="2-糖豆的设计与实现"><a href="#2-糖豆的设计与实现" class="headerlink" title="2. 糖豆的设计与实现"></a>2. 糖豆的设计与实现</h2><h3 id="2-1-架构"><a href="#2-1-架构" class="headerlink" title="2.1 架构"></a>2.1 架构</h3><p>糖豆整体推荐框架，从离线，近线，在线三套计算流程组合而成。在线流程基于Spark Streaming框架实现，部署在近线集群。 在线推荐框架实时根据用户行为，生成实时推荐列表，从而满足用户瞬时兴趣，提高推荐系统的推荐新鲜度。简单架构图如下:<br><img src="http://upload-images.jianshu.io/upload_images/20467-22dfa6355957facd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="糖豆实时架构.png"></p>
<h3 id="2-2-基于Spark-Streaming的实现"><a href="#2-2-基于Spark-Streaming的实现" class="headerlink" title="2.2 基于Spark Streaming的实现"></a>2.2 基于Spark Streaming的实现</h3><p>####2.2.1. 计算流程<br>实时计算流程如下图所示:</p>
<p><img src="http://gitlab.tangdou.com/data/doc/raw/master/recy/img/realtime.png" alt="实时计算流程图"><br>分解步骤：</p>
<ol>
<li>Spark Streaming 读取Kafka，原始日志ETL</li>
<li>提取用户隐式反馈，生成候选集tuple (uid,vid)</li>
<li>每天凌晨会将离线计算好的ItemCF模型结果集导入Redis。itemcf数据结构是一个similarity vid list。</li>
<li>实时维护看过视频set,对看过视频的处理候选集tuple过滤该用户看过的视频</li>
<li>实时更新推荐过视频set,候选集tuple过滤当天已经被推荐过的视频</li>
<li>候选集写入Redis推荐list</li>
</ol>
<p>python实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">print</span> sys.argv</span><br><span class="line">    reload(sys)</span><br><span class="line">    sys.setdefaultencoding(<span class="string">'utf-8'</span>)</span><br><span class="line">    sc = SparkContext(appName=<span class="string">"real_time_etl"</span>)</span><br><span class="line">    <span class="comment">#20秒</span></span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">15</span>)</span><br><span class="line">    brokers = <span class="string">"kafka-servers:9092"</span></span><br><span class="line">    topic = <span class="string">"logstash"</span></span><br><span class="line">    <span class="comment">#读取kafka</span></span><br><span class="line">    kvs = KafkaUtils.createDirectStream(ssc, [topic], &#123;<span class="string">"metadata.broker.list"</span>: brokers&#125;)</span><br><span class="line">   <span class="comment">#解析日志、过滤无关数据、读取相似视频</span></span><br><span class="line">    lines = kvs.map(<span class="keyword">lambda</span> x : readJson(x[<span class="number">1</span>])).filter(<span class="keyword">lambda</span> x: x <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>).map(<span class="keyword">lambda</span> x: getTopkfromRedis(x))</span><br><span class="line">    <span class="comment">#lines.pprint()</span></span><br><span class="line">     <span class="comment">#写入推荐结果</span></span><br><span class="line">    lines.foreachRDD(<span class="keyword">lambda</span> rdd: list2Redis(rdd))  </span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<p>####2.2.2 监控<br>部署在集群Master节点的监控脚本会每30s扫描一次实时计算代码进程，如果发现进程被failed，会自动拉起实时计算Spark Steaming进程。如果进程拉起失败会触发邮件、短信报警<br>  <figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#! /bin/sh</span></span><br><span class="line"></span><br><span class="line">MOBILE=<span class="string">"your phone numbers"</span></span><br><span class="line">RT_HOME=/home/realtime/recommend.py</span><br><span class="line"></span><br><span class="line">DIR=/data/rtdamon</span><br><span class="line">PID_FILE=<span class="variable">$DIR</span>/.run/rt-litetl-damon.pid</span><br><span class="line">LOG_FILE=<span class="variable">$DIR</span>/.<span class="built_in">log</span>/rt-litetl-damon.log</span><br><span class="line">t=$(date -d <span class="string">"today"</span> +<span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$PID_FILE</span> <span class="variable">$LOG_FILE</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ -e <span class="string">"<span class="variable">$PID_FILE</span>"</span> ];</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">        pid=`cat <span class="variable">$PID_FILE</span>`</span><br><span class="line">        <span class="built_in">echo</span> <span class="variable">$pid</span></span><br><span class="line">        damon_process_exists=`ps v -p <span class="variable">$pid</span> | grep <span class="string">"rt-litetl-damon.sh"</span> | grep -v grep|grep -v \&lt;defunct\&gt; `</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"damon process exists : <span class="variable">$process_exists</span>"</span></span><br><span class="line">        <span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$damon_process_exists</span>"</span> ]</span><br><span class="line">        <span class="keyword">then</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">"Process rt-litetl-damon.sh is running! <span class="variable">$t</span>"</span> &gt;&gt; <span class="variable">$LOG_FILE</span></span><br><span class="line">                <span class="built_in">exit</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">pid=$$</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$pid</span>"</span> &gt; <span class="variable">$PID_FILE</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> :</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        process_exists=`ps -ef|grep <span class="string">"<span class="variable">$RT_HOME</span>"</span>|grep <span class="string">"spark"</span>|grep -v grep|wc -l`</span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"process exists : <span class="variable">$process_exists</span>"</span> &gt;&gt;<span class="variable">$LOG_FILE</span></span><br><span class="line">        <span class="keyword">if</span> [ <span class="string">"<span class="variable">$process_exists</span>"</span> == <span class="string">"0"</span> ]; <span class="keyword">then</span></span><br><span class="line">		</span><br><span class="line"></span><br><span class="line">/hadoop/spark/bin/spark-submit  --master yarn --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.1.0 --py-files /hadoop/user/rt/redis.zip --num-executors 10 --executor-cores 7 --executor-memory 6g /home/realtime/recommend.py&gt;&gt;/data/rtlog/rtrecommed.log  2&gt;&amp;1 &amp;</span><br><span class="line">	/usr/bin/php -f /data/rtdamon/yunsms.class.php <span class="string">"<span class="variable">$MOBILE</span>"</span> <span class="string">"recommend.py"</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">"realtime recommendation process already restarted at <span class="variable">$t</span>"</span> &gt;&gt; <span class="variable">$LOG_FILE</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#sleep `expr 3600 \* 3`</span></span><br><span class="line">        sleep `expr 60 \* 1`</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure></p>
<h3 id="2-3-收益"><a href="#2-3-收益" class="headerlink" title="2.3 收益"></a>2.3 收益</h3><p>根据我们的AB测试数据来看，整体CTR提升25%。用推荐系统的A版对比无推荐的B版，用户观看时长提升47%。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/20467-527ed03f8c6933aa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="recabdata.png"></p>
<h3 id="3-问题与改进"><a href="#3-问题与改进" class="headerlink" title="3.  问题与改进"></a>3.  问题与改进</h3><ol>
<li>较多代码逻辑集中在Redis。目前Redis无灾备措施，同时IO和负载也会出现Peak。</li>
<li>Spark Streaming 目前实时级别在分钟级。需要升级成storm的秒、毫秒级别。</li>
<li>需要用户点击等行为才会生产数据，容易召回不足。</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/24/糖豆实时推荐系统设计与实现/" data-id="cjxdyu82m000b2owfl1ma4tke" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/recommendation-system/">recommendation system</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-推荐资料汇总与解说" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/24/推荐资料汇总与解说/" class="article-date">
  <time datetime="2019-06-24T13:56:11.000Z" itemprop="datePublished">2019-06-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/24/推荐资料汇总与解说/">推荐资料汇总与解说</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近和好几位朋友讨论推荐系统的搭建和升级，暂时没空将全部实践过的部分都写出来，但是看过的资料和论文倒是一气呵成地梳理出来，对想从零搭建一个推荐系统或者对推荐系统现有效果不满希望升级的同学来说，可能有用，也可能没用，你都已经看过。</p>
<h2 id="1-架构部分"><a href="#1-架构部分" class="headerlink" title="1. 架构部分"></a>1. 架构部分</h2><ol>
<li><p>头条首席架构师曹欢欢的分享，为了让业界了解头条的算法，比较清晰扼要，突出重点，没有讲最新的内容。但是核心都涉及到，<strong>值得反复研读</strong>。基本上绝大部分推荐系统架构和头条一致。当中的差异主要体现在实时框架上，头条继承百度系的搜索架构思想，喜欢用倒排索引来实现很多部件。阿里、腾讯更加倾向于类似于Strom的流式计算加上KV存储的方式。个人认为并无高低之分，看架构师、负责人喜好和团队技能分布。<br><a href="https://36kr.com/p/5114077.html?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io" target="_blank" rel="noopener">36氪首发 | 今日头条推荐算法原理全文详解</a><br><a href="https://www.leiphone.com/news/201707/YYoc8r9MWsBy2QAC.html?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io" target="_blank" rel="noopener">今日头条成功的核心技术秘诀是什么？深度解密个性化资讯推荐技术
</a></p>
</li>
<li><p>爱奇艺的推荐架构演化，有整体架构、算法模型的升级进化过程，从算法产品的角度来讲述不同的算法模型演化和效果过程，是一个值得follow的演化路径。可能在爱奇艺的数据上，这些模型的效果如此好。实际上，在大量其他公司的实践和paper，可能同样的算法模型不一定做的出来一样的效果。<br><a href="https://mp.weixin.qq.com/s/lUP2BehOh7KczR3WRnOqFw?" target="_blank" rel="noopener">爱奇艺个性化推荐排序实践</a></p>
</li>
<li><p>业界解读youtube推荐算法模型，这篇文章值得去看的是，如何设计观测指标，如何评估内容。<br><a href="https://mp.weixin.qq.com/s?__biz=MzAwNDI4ODcxNA==&mid=2652245506&idx=1&sn=99fd0a8836314dfbc0bdbe71d1109c9d&chksm=80cc97a7b7bb1eb1dc47baac570039435dd0298fdcb1f868d734b8f45fb707b6634198c42f61&scene=0&key=e885a9508b54ccdf80eb2fafd7c3f32f87741a9b63e969747b2adbdb3959d798cf598c14c3f134104d42ce80ddcdb002420b960acbbba9715faf5adeb41a4d530ea7de8f879adfd4532f34d21d58d8a7&ascene=0&uin=Mjg5ODcwODI4MQ%253D%253D&devicetype=iMac+MacBookPro12%252C1+OSX+OSX+10.11.5+build(15F34)&version=12020810&nettype=WIFI&fontScale=1" target="_blank" rel="noopener">从YouTube算法论文反推其推荐机制</a></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NDQ3NTkwMA==&mid=2650141414&idx=1&sn=74e57b6bd1cab52b6e0491ec11794c34&scene=1&srcid=0714o1JdGFKi6L5fNCOR6rqV&key=77421cf58af4a65330f0d4492fbefbeb0b926c66ff7a0fcbec6162eb77eddf28c0b536e8ccbd2d56732a3f306209f839&ascene=0&uin=Mjg5ODcwODI4MQ%253D%253D&devicetype=iMac+MacBookPro12%252C1+OSX+OSX+10.11.5+build(15F34)&version=11020201&pass_ticket=%252F6rqIBONsFaqeo1eh5A8k7McSMokX8AKqh9pZFbV7xIGH6LAG%252BWiHPU2S6zEvkcQ" target="_blank" rel="noopener">如何破解YouTube视频推荐算法？</a></p>
</li>
<li><p>饿了么的推荐架构，包含一部分LBS的领域知识，整体架构也比较清晰。推进的路径也不错，EE的应用也重视的很好。<br><a href="https://mp.weixin.qq.com/s/OGYVBG0l1wuLAVqs6SfW3w" target="_blank" rel="noopener">回顾·外卖推荐算法中有哪些机制与手段？</a></p>
</li>
</ol>
<ol start="5">
<li>糖豆的实践，第一期比较稚嫩，但是0到1是gain最大的时候，极少人力就能快速上线，建立良好的评估基线极其重要。包括了实时、深度学习、强化学习等内容，有些零散，但是比较直接面对具体的一个推荐系统阶段。上述的文章一般不会这么详细。<br><a href="https://www.jianshu.com/p/3b3c070de906" target="_blank" rel="noopener">糖豆推荐系统第一期开发与评估报告</a><br><a href="https://www.jianshu.com/p/23098598a58e" target="_blank" rel="noopener">糖豆实时推荐系统设计与实现</a><br><a href="https://www.jianshu.com/p/6938f1ef29bc" target="_blank" rel="noopener">单步强化学习在糖豆推荐系统的应用</a><br><a href="https://www.jianshu.com/p/b441435c9c89" target="_blank" rel="noopener">深度学习于糖豆推荐应用–图片模糊识别</a><br>第二期后续的工作我一直没空写,以后有空写一下，大概包括算法演进、语义挖掘、用户画像、深度学习的尝试等。</li>
</ol>
<h2 id="2-模型部分"><a href="#2-模型部分" class="headerlink" title="2. 模型部分"></a>2. 模型部分</h2><h4 id="2-1-基础的推荐模型"><a href="#2-1-基础的推荐模型" class="headerlink" title="2.1 基础的推荐模型"></a>2.1 基础的推荐模型</h4><p>包括<strong>基于流行度</strong>，<strong>基于协同过滤</strong>，<strong>基于内容</strong>等模型。<br>这些模型都比较简单却非常有效，一般作为候选集的选择或者粗排去使用或者推荐系统初期模型，这些都是非CTR预估模型。<br><a href="https://towardsdatascience.com/collaborative-filtering-based-recommendation-systems-exemplified-ecbffe1c20b1" target="_blank" rel="noopener">Collaborative Filtering based Recommendation Systems exemplified</a><br><a href="https://www.analyticsvidhya.com/blog/2015/08/beginners-guide-learn-content-based-recommender-systems/" target="_blank" rel="noopener">Beginners Guide to learn about Content Based Recommender Engines</a></p>
<hr>
<p>####2.2    LR及其推广模型</p>
<ul>
<li><p><strong>LR</strong><br>它是基线模型，后续所有模型都要和它对比。离线对比的指标主要是AUC，logloss，RMSE，NDCG等等，最好都看，个人主要看AUC。我这里面介绍就简单讲AUC为主。LR模型虽然非常简单，但是特征工程+LR基本能解决大部分推荐的问题。它的缺点当然非常多，包括学习能力有效，需要领域知识实现特征之间的交叉组合等等。但是后续会介绍的大量模型里面(在我看过的paper里面),没有一个模型敢说超越LR 20%以上的(公开数据集)。LR AUC最好能做到0.76~0.78，depend on 数据集。一般建议把LR AUC起码做到0.72左右，再进行下一步模型升级。<br><a href="http://rnowling.github.io/data/science/2016/10/20/lr-hashing-recsys.html" target="_blank" rel="noopener">Recommendation System Using Logistic Regression and the Hashing Trick
</a></p>
</li>
<li><p><strong>FTRL</strong><br>它是谷歌提出来的在线学习模型，它实际上是对LR的GD过程做了在线的算法和实现优化。适合于极大级别的实时预测。<br><a href="https://research.google.com/pubs/archive/37013.pdf" target="_blank" rel="noopener">Follow-the-Regularized-Leader and Mirror Descent:<br>Equivalence Theorems and L1 Regularization</a></p>
</li>
<li><p><strong>MLR</strong><br>它是阿里盖坤团队提出的LR模型的推广。MLR大概就是采用一个级联器组合了LR，能够通过空间分片的方法来逼近任何高维空间的非线性分类面。在阿里妈妈的广告方面应用比较广泛。<br><a href="https://arxiv.org/pdf/1704.05194.pdf" target="_blank" rel="noopener">Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction
</a></p>
</li>
</ul>
<hr>
<h4 id="2-3-基于隐变量的模型"><a href="#2-3-基于隐变量的模型" class="headerlink" title="2.3 基于隐变量的模型"></a>2.3 基于隐变量的模型</h4><ul>
<li><p><strong>SVD</strong><br>矩阵分解的方法，它是多年前推荐系统的圣杯– Netflix Prize最终获奖模型 – SVD++集成RBM，比Netflix当时的模型提升了10%，这是一个恐怖的提升。它思想是用户对物品喜好程度(隐向量)可以用用户-物品大矩阵来表达，通过已知的用户对物品行为推断用户对其他为接触物品的喜好程度。具体算法实现就是将大矩阵分解为user 和item两个小矩阵，用最小二乘法求解得到。但是它在实现上比较麻烦，Spark对SVD的实现性能不算太好。同时模型解释性也比较差。<br><a href="http://buzzard.ups.edu/courses/2014spring/420projects/math420-UPS-spring-2014-gower-netflix-SVD.pdf" target="_blank" rel="noopener">Netflix Prize and SVD</a><br><a href="https://spark.apache.org/docs/2.0.1/ml-collaborative-filtering.html" target="_blank" rel="noopener">Spark SVD</a></p>
</li>
<li><p><strong>FM</strong><br>因子分解机的方法，纯粹的矩阵分解无法融入用户、物品的特征。FM能够结合显性变量和隐性变量，模型能够有效表达特征组合(实际应用基本只是两两组合)，允许稀疏高维特征空间的参数估计。<br><a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf" target="_blank" rel="noopener">Factorization Machines</a><br><a href="https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html" target="_blank" rel="noopener">深入FFM原理与实践</a></p>
<p>FM的改进版本包括FFM等等，都在百度、美团等公司的广告、推荐等系统广泛使用。</p>
</li>
</ul>
<hr>
<h4 id="2-4-Tree-based-ensemble模型"><a href="#2-4-Tree-based-ensemble模型" class="headerlink" title="2.4 Tree-based ensemble模型"></a>2.4 Tree-based ensemble模型</h4><ul>
<li><p><strong>GBDT+LR</strong><br>它是Facebook提出的经典模型，最核心地方是省去人工做特征工程的部分。AUC 有可能能做到0.8。这个模型有些人直接简化成用GBDT来预测，不同场景表现不太一样，但基本差异不大。它的缺点，其实就是Tree model的缺点，整体来讲model是历史数据的记忆，推广性较差。<br><a href="http://quinonero.net/Publications/predicting-clicks-facebook.pdf" target="_blank" rel="noopener">Practical Lessons from Predicting Clicks on Ads at<br>Facebook</a><br>很多人喜欢用XGBoost框架，个人认为Spark的GBDT、LightGBM也差别不大。</p>
</li>
<li><p><strong>GBDT+FM</strong><br>基于FB这个思路,FM比LR能更好表达二维组合交叉特征，GBDT+FM能够在大规模稀疏特征空间有不错的性能表现。方法在Kaggle竞赛中拿到不错的名次。前些年ensemble框架是极其流行，工业界应用也是非常广泛。<br><a href="https://github.com/owenzhang/kaggle-avazu" target="_blank" rel="noopener">2nd place solution for Avazu click-through rate prediction competition
</a></p>
</li>
</ul>
<hr>
<h4 id="2-5-基于深度学习的模型"><a href="#2-5-基于深度学习的模型" class="headerlink" title="2.5 基于深度学习的模型"></a>2.5 基于深度学习的模型</h4><ul>
<li><p><strong>Wide &amp; Deep Model</strong><br>以上基本都是传统ML的方法，它们有极其大量的变种和改进，但是思路没有本质变化。Wide &amp; Deep Model 是google 提出的基于深度学习框架的CTR预估模型，它在youtube应用效果不错。它作用不止于此，属于用深度学习打开了传统CTR预估的大门，同时集成了传统ML和深度学习的优点。后续有无数的基于wide &amp; deep 思想的深度学习的CTR预估模型算法。<br><a href="https://arxiv.org/pdf/1606.07792.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1606.07792.pdf</a></p>
</li>
<li><p><strong>DCN</strong><br>Google提出的深度组合网络，在DNN基础上通过加入cross网络，能够在每层自动化进行特征组合。效果上主要对比了LogLoss，比LR有显著提升，比DNN也有一定提升。<br><a href="https://arxiv.org/pdf/1708.05123.pdf" target="_blank" rel="noopener">Deep &amp; Cross Network for Ad Click Predictions</a></p>
</li>
</ul>
<ul>
<li><p><strong>DIN</strong>。
深度兴趣网络，阿里提出来的深度学习CTR预估模型，应用在阿里妈妈的广告预估上。主要是通过embedding的学习和多层感知机组合在端到端学习里面。前者刻画了淘宝用户的多重兴趣，后者将多种行为聚合成单一向量，据说效果非常好。<br><a href="https://arxiv.org/pdf/1706.06978.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.06978.pdf</a></p>
</li>
<li><p><strong>DeepFM</strong>。
FM是不错的ML模型，用深度学习来结合，得到一个更强的模型。<br><a href="https://www.ijcai.org/proceedings/2017/0239.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2017/0239.pdf</a></p>
</li>
</ul>
<h2 id="3-其他部分"><a href="#3-其他部分" class="headerlink" title="3. 其他部分"></a>3. 其他部分</h2><p>可以看到推荐系统不是一个简单的工程，涉及到内容、产品策略、客户端、服务端、大数据、推荐工程、推荐算法、评估体系等等一系列，这些组件环环相扣，存在大量变量和组合，也有漫长的迭代周期，相信每个公司在实践过程中有大量的独特的体验和收获。过去的关于所有这些推荐系统的建设经验局限于时间和工作因素，没有全部都写完， 以下还有补充两个部分，抛砖引玉 。</p>
<p>•    AB测试平台是线上评估的必须。<br><a href="https://www.jianshu.com/p/2fcdd25d3499" target="_blank" rel="noopener">https://www.jianshu.com/p/2fcdd25d3499</a></p>
<p>•    大数据的埋点非常重要，准备好最充分的数据原材料<br><a href="https://www.jianshu.com/p/d45235b51601" target="_blank" rel="noopener">https://www.jianshu.com/p/d45235b51601</a></p>
<p>本来还有实验部分、评估部分、实战内容，但是我当时比较困了，就不想写了。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/24/推荐资料汇总与解说/" data-id="cjxdyu82k00092owf0y5sk9t9" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/recommendation-system/">recommendation system</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-一站式机器学习平台资源介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/24/一站式机器学习平台资源介绍/" class="article-date">
  <time datetime="2019-06-24T13:45:23.000Z" itemprop="datePublished">2019-06-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/24/一站式机器学习平台资源介绍/">一站式机器学习平台资源介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="1-缘起"><a href="#1-缘起" class="headerlink" title="1. 缘起"></a>1. 缘起</h2><p>近日和公司同学分享了推荐系统的构建、策略、算法等相关内容。由于课程长度缘故，来不及和大家实践一下，如何实现一个简单推荐系统的CTR预估模型。于是，课后想分享一个基于Jupyter Notebook的demo，发现居然一时间找不到国内可以用于分享、协作、运行的机器学习平台。通过google找到了几个一站式的机器学习平台，它们的特点都是基于Jupyter Notebook 构建的，都能够在上面开发、调试、训练、运行、分享机器学习相关的代码集和数据集。</p>
<h3 id="1-1-Jupyter-NoteBook"><a href="#1-1-Jupyter-NoteBook" class="headerlink" title="1.1 Jupyter NoteBook"></a>1.1 Jupyter NoteBook</h3><p>首先回顾介绍Jupyter Notebook，根据官网的介绍，Jupyter是一种可以允许用户创建、分享代码、公式、可视化等富文本的web 应用。用户通常在上面做数据清洗、数据转换、数值计算、统计建模、机器学习等等。可以说Jupyter是数据科学家最常用、最好用的工具之一，可以快速的对数据处理、可视化、建模，可以说是数据科学的事实标准编辑器了。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20467-bea8f99b745e6efe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Jupyter官网介绍"></p>
<h3 id="1-2-免费平台资源"><a href="#1-2-免费平台资源" class="headerlink" title="1.2 免费平台资源"></a>1.2 免费平台资源</h3><table>
<thead>
<tr>
<th>平台名称</th>
<th>计算核心</th>
<th>核心运行时</th>
<th>内存</th>
<th>存储</th>
<th>支持语言</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://colab.research.google.com/" target="_blank" rel="noopener">google colab</a></td>
<td>CPU 2cores /GPU tesla k80s 1core/TPU  8cores</td>
<td>12个小时</td>
<td>12G</td>
<td>50G</td>
<td>py2,py3</td>
</tr>
<tr>
<td><a href="https://www.kaggle.com/kernels" target="_blank" rel="noopener">kaggle kernel</a></td>
<td>CPU 4 cores/GPU 2cores</td>
<td>6个小时</td>
<td>16G/12G</td>
<td>1G</td>
<td>py2,py3</td>
</tr>
<tr>
<td><a href="https://notebooks.azure.com/" target="_blank" rel="noopener">Azure Notebooks</a></td>
<td>未知</td>
<td>未知</td>
<td>4GB</td>
<td>1G</td>
<td>py2,py3,R,F#</td>
</tr>
<tr>
<td>一般而言，你用GPU训练一个模型，平台提供的内存资源会相应减少一些。在<a href="https://medium.com/datadriveninvestor/comparing-gpu-and-tpu-training-performance-on-google-colaboratory-c1e54e26993f" target="_blank" rel="noopener">一些评测</a>中，google的GPU比TPU性能稍微强劲些。总体而言，Google colab更加适合较为大型的模型的训练。</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="1-3-收费平台资源"><a href="#1-3-收费平台资源" class="headerlink" title="1.3 收费平台资源"></a>1.3 收费平台资源</h3><p>不同的平台有不同的收费标准，基本上可以看做是函数计算的云服务在售卖，本质上比 AI as services 底层一点，基础收费方案，大约在一个月10刀左右。</p>
<h4 id="floydhub"><a href="#floydhub" class="headerlink" title="floydhub"></a><a href="https://www.floydhub.com/" target="_blank" rel="noopener">floydhub</a></h4><p>如下图所示，可以看到，主力方案，基本提供100G存储，使用的是NVIDIA的Tesla K80或者V100.但是需要注意这些资源都是抢占式的，运行时可以长达7天，一般能用上完整的GPU，内存在60G左右。<br><img src="https://upload-images.jianshu.io/upload_images/20467-4f5975fc281c37ad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="floyhub 价格方案"></p>
<p>####<a href="https://www.paperspace.com" target="_blank" rel="noopener">paperspace</a><br>如下图所示,paperspace使用的内核和内存方案类似于floyhub，但是有一定GPU并发，以及notebook的限制。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20467-f03eab9ab69bfcd2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="paperspace 价格方案"></p>
<p>当然，你还可以购买一台GPU Server 部署直接的Jupyter notebook。amazon,google,azure都有自己方案，那么价格就远贵于AI as service的供应商服务啦。</p>
<h2 id="2-Google-Colab"><a href="#2-Google-Colab" class="headerlink" title="2.Google Colab"></a>2.Google Colab</h2><p>google colab 是以上介绍平台中，最适合个人开发者的。它不仅仅是提供切实可用的计算、存储资源，它的文件是默认存储在google drive中，也能够集成GitHub做版本控制。同时还可以安装第三方python包，读写第三方的数据源，还能很轻松分享notebook文件。</p>
<p>打开首个google colab notebook，如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20467-25d65dfaad6f3326.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="google colab"></p>
<p>可以通过修改设置，选择你的代码运行环境，google支持py2和py3，硬件加速支持CPU、GPU、TPU。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20467-d3f06b9b44557aee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="修改运行时"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/20467-7258f834c4b39cfc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="选择后端引擎"></p>
<p>可以通过代码查看底层所分配的硬件资源。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.client <span class="keyword">import</span> device_lib</span><br><span class="line">device_lib.list_local_devices()</span><br></pre></td></tr></table></figure>

<p><img src="https://upload-images.jianshu.io/upload_images/20467-a54e71cd67ec536a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="硬件资源"></p>
<p>可以通过 google colab 的库，上传本地文件。文件会临时存储在content目录下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> files</span><br><span class="line">uploaded = files.upload()</span><br><span class="line"><span class="keyword">for</span> fn <span class="keyword">in</span> uploaded.keys():</span><br><span class="line">  print(<span class="string">'User uploaded file "&#123;name&#125;" with length &#123;length&#125; bytes'</span>.format(</span><br><span class="line">      name=fn, length=len(uploaded[fn])))</span><br><span class="line"><span class="keyword">for</span> name, data <span class="keyword">in</span> uploaded.items():</span><br><span class="line">  <span class="keyword">with</span> open(name, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(data)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'saved file'</span>, name)</span><br></pre></td></tr></table></figure>

<p><img src="https://upload-images.jianshu.io/upload_images/20467-e916a77b7f3b7f17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="上传文件"></p>
<p>简而言之，google colab是很好的一站式机器学习平台，值得信赖、使用。google colab拥有的能力远超文章所列举，只待各位good coding~</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/24/一站式机器学习平台资源介绍/" data-id="cjxdyu82f00032owfmm751umf" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ml/">ml</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-DeepFM理论与其应用" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/24/DeepFM理论与其应用/" class="article-date">
  <time datetime="2019-06-24T13:41:14.000Z" itemprop="datePublished">2019-06-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/24/DeepFM理论与其应用/">DeepFM理论与其应用</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>DeepFM[1]是哈工大Guo博士在华为诺亚实验室实习期间，提出的一种深度学习方法，它基于Google的经典论文Wide&amp;Deep learning 基础上，通过将原论文的wide部分–LR部分替换成FM[4]，从而改进了原模型依然需要人工特征工程的缺点，得到一个end to end 的深度学习模型。DeepFM在企业数据集(华为应用商店)和公开数据集(<a href="https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz" target="_blank" rel="noopener">criteo</a>)上都取得不错的效果，目前该方法在不少互联网公司的推荐、广告系统中得到了较为广泛的应用。</p>
<h1 id="1-CTR预估中的特征分析"><a href="#1-CTR预估中的特征分析" class="headerlink" title="1. CTR预估中的特征分析"></a>1. CTR预估中的特征分析</h1><p>在CTR预测中，挖掘用户行为中的隐藏特征以及它们之间的交叉特征已经成为推荐算法中最核心的一部分。华为通过对自己应用市场的用户进行行为分析，得到以下两个重要的结论。</p>
<ol>
<li>用户喜欢在等待外卖送达的时段下载APP。它说明时间和APP类别的二维特征交叉是一个有效的特征输入信号。</li>
<li>青少年的男性用户喜欢下载射击或者RPG游戏APP。它则说明年龄、性别、APP类别的三维特征交叉也是一个有效的输入信号。</li>
</ol>
<p>我们可以看到，CTR预估中主要挑战是有效对特征交互建模，有些特征交互可以很容易理解，因此特征工程的专家可以人工设计出来。然而，绝大部分特征都是隐藏在数据背后，难以形成专家的先验知识，只能通过机器学习自动生成。由于实际应用中使用到的特征非常多(原始特征经常有几十到上百维)，就算是简单的特征交互，专家其实也无法对全部特征交叉进行有效建模。</p>
<p>广义线性模型实现简单、性能好，但是缺乏学习特征交叉的能力，通常在工业实践中会人工做特征工程来解决这个问题。FM采用隐向量(latent vector)的内积作为对特征交叉的建模方法，具有很好的效果，FM在深度学习时代之前是CTR预估最为广泛应用的一种算法，它在实践中通常只会利用二维的特征交叉。</p>
<p>总而言之，用户行为的特征维度是高度复杂的，无论是低维还是高维的特征交叉都会起到重要的作用。根据Google wide&amp;deep model，它在建模过程中同时考虑了低维和高维的特征交叉，以提升模型的效果。</p>
<h1 id="2-深度学习在CTR预估的进展"><a href="#2-深度学习在CTR预估的进展" class="headerlink" title="2.深度学习在CTR预估的进展"></a>2.深度学习在CTR预估的进展</h1><p>Google的Wide &amp; Deep Learning for Recommender Systems[2]是深度学习应用于推荐、广告等CTR领域的重要论文。过去几年，神经网络已经在图像、音频等领域得到广泛应用，而由于推荐、广告等领域由于数据的稀疏性、离散性，无法直接套用传统的深度学习模型。</p>
<p>基于深度学习的思想，Google 提出一种深度模块和广度模块结合的神经网络模型。Wide端使用常见的LR(FTRL[5]实现)模型，将常见的离散特征、低维特征组合作为输入，实现了模型的记忆能力。换句话说，模型能够很好记住用户的喜好，给用户推荐 <strong>常见喜好</strong> 的内容。Deep端将离散特征通过embedding方法转化成稠密特征向量输入，实际上实现了tag向量的模糊查询，扩充了模型的泛化能力。换句话说，模型能够更好理解用户-物品之间内在的高维关系，给用户推荐 <strong>罕见但是可能喜好</strong> 的内容，破解“信息茧房”的问题。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20467-ebb5c66354aea1a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Wide&amp;Deep"></p>
<p>###2.1 稀疏特征的优点：</p>
<ul>
<li>LR, DNN在底层还是一个线性模型，但是现实生活中，标签y与特征x之间较少存在线性关系，而往往是分段的。以”点击率 ~ 历史曝光次数” 之间的关系为例，之前曝光过1、2次的时候，“点击率 ~ 历史曝光次数”之间一般是正相关的，再多曝光1、2次，用户由于好奇，没准就点击了；但是，如果已经曝光过8、9次了，由于用户已经失去了新鲜感，越多曝光，用户越不可能再点，这时“点击率 ~ 历史曝光次数”就表现出负相关性。因此，categorical特征相比于numeric特征，更加符合现实场景。</li>
<li>推荐、搜索一般都是基于用户、商品的标签画像系统，而标签天生就是categorical的</li>
<li>稀疏的类别/ID类特征，可以稀疏地存储、传输、运算，提升运算效率。</li>
</ul>
<p>###2.2 稀疏特征的缺点：</p>
<ul>
<li>稀疏的categorical/ID类特征，也有着单个特征表达能力弱、特征组合爆炸、分布不均匀导致受训程度不均匀的缺点。</li>
<li>FTRL 充分输入的稀疏性在线更新模型，训练出的模型也是稀疏的，便于快速预测。</li>
<li>Parameter Server，充分利用特征的稀疏性，不必在各机器之间同步全部模型，而让每台机器“按需”同步自己所需要的部分模型权重，“按需”上传这一部分权重的梯度。</li>
<li>TensorFlow Feature Column类，除了一个numeric_column是处理实数特征的，其实的都是围绕处理categorical特征的，封装了常见的分桶、交叉、哈希等操作。</li>
</ul>
<p>总而言之：</p>
<ul>
<li><p>Wide for Memorization，wide侧记住的是历史数据中那些常见、高频的模式。根据人工经验、业务背景，将有价值的、显而易见的特征及特征组合输入wide侧。</p>
</li>
<li><p>Deep for Generation，deep侧通过embedding将tag向量化，变tag的精确匹配，为tag向量的模糊查询，因而模型具备良好的“扩展”能力。</p>
</li>
</ul>
<p>Wide &amp; Deep模型应用Google Play的数据，它包含超过10亿活跃用户以及上百万的app行为。在线实验显示Wide&amp; Deep model 有效提升了App的购买率。代码开源集成到了TensorFlow内，调用DNNLinearCombinedClassifier 这个estimator就可以。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">estimator = DNNLinearCombinedClassifier(</span><br><span class="line">    <span class="comment"># wide侧设置</span></span><br><span class="line">    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],</span><br><span class="line">    linear_optimizer=tf.train.FtrlOptimizer(...),</span><br><span class="line">    <span class="comment"># deep侧设置</span></span><br><span class="line">    dnn_feature_columns=[</span><br><span class="line">        categorical_feature_a_emb, categorical_feature_b_emb,</span><br><span class="line">        numeric_feature],</span><br><span class="line">    dnn_hidden_units=[<span class="number">1000</span>, <span class="number">500</span>, <span class="number">100</span>],</span><br><span class="line">    dnn_optimizer=tf.train.ProximalAdagradOptimizer(...),</span><br><span class="line">    <span class="comment"># warm-start 设置</span></span><br><span class="line">    warm_start_from=<span class="string">"/path/to/checkpoint/dir"</span>)</span><br></pre></td></tr></table></figure>

<p>除了Wide and Deep 以外还有数篇文章探索深度学习在CTR预估领域的应用，其中包括采用FM对特征做初始化处理的FNN[3]。FNN通过<br>它的模型如下图所示：<br><img src="https://upload-images.jianshu.io/upload_images/20467-5073d50f8ca9d69f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="FNN"></p>
<p>它主要缺点在于，embedding 后的特征可能会被FM模型过度影响。使用FM对特征做预处理的做法，可能影响了模型的性能和效率。它只能刻画高维的特征交互，而不像Wide &amp; Deep那样高维和低维特征交叉都能刻画到。</p>
<h1 id="3-DeepFM核心思想"><a href="#3-DeepFM核心思想" class="headerlink" title="3.DeepFM核心思想"></a>3.DeepFM核心思想</h1><p>DeepFM将Wide and Deep 模型中的Wide侧的LR替换成FM，克服了原有模型依然需要对低维特征做特征工程的缺点，实现了一个无需任何人工特征工程的end to end 模型。DeepFM在wide侧和deep侧共享了embedding的特征向量。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20467-1dd9db3b7d37f9cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="DeepFM架构"></p>
<p>可以看到DeepFM的数学形式化：<br>$$y=sigmod(yFM +yDNN)$$</p>
<p>yFM 是FM组件的输出，yDNN是深度组件的输出结果。FM组件能够捕获一维特征的同时，还能很好捕获二维稀疏组合特征。如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20467-b59ab105e5779843.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="FM数学公式"></p>
<p>yDNN旨在学习高维特征组合，和图像、音频的稠密数值张量不同的是，在推荐系统中DNN模型的数据输入通常都是非常稀疏的张量，所以在技术上一般会采用embedding层来压缩数据空间维度。</p>
<p>DeepFM 在企业数据集(华为应用商店)和公开数据集(criteo)进行多次实验，采用AUC和LogLoss来评估效果。具体效果如下图所示：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20467-23287daf44ccaba0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="评估结果"><br>DeepFM在公开数据上，比LR&amp;DNN AUC提升了一百多个基点，是一个非常好的改进。</p>
<h1 id="4-DeepFM重要参数"><a href="#4-DeepFM重要参数" class="headerlink" title="4. DeepFM重要参数"></a>4. DeepFM重要参数</h1><p>这篇文章有趣的部分是探索与分享整个模型的多个超参，从而分析如何得到一个更好效果的模型。</p>
<p>###4.1 激活函数<br>relu 函数和 tanh 函数比sigmod函数效果更好。</p>
<h3 id="4-2-Dropout"><a href="#4-2-Dropout" class="headerlink" title="4.2 Dropout"></a>4.2 Dropout</h3><p>下图效果显示：采用适合的随机性能够加强模型的鲁棒性，建议采用dropout比率在0.6~0.9之间。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20467-73b04c15ae8f1557.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Dropout"></p>
<h3 id="4-3-每层神经元个数"><a href="#4-3-每层神经元个数" class="headerlink" title="4.3 每层神经元个数"></a>4.3 每层神经元个数</h3><p>建议采用200~400个神经元能够给模型更好效果。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20467-c7ea72cfd83fe4db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="神经元个数"></p>
<h3 id="4-4-隐含层数量"><a href="#4-4-隐含层数量" class="headerlink" title="4.4 隐含层数量"></a>4.4 隐含层数量</h3><p>增加隐含层的数量能够一定程度提升模型效果，但是要注意过拟合的情况。建议3~5个隐藏层为妙。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20467-dfc8cc9e0d867a79.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="隐藏层数量"></p>
<h3 id="4-5-网络结构"><a href="#4-5-网络结构" class="headerlink" title="4.5 网络结构"></a>4.5 网络结构</h3><p>文章中测试了四种深度网络结构，不变型(constant),增长型(increasing),衰减型(decreasing),钻石型(diamond)。文章保证四种网络结构神经元总量一致，采用三层隐藏层，从而四种形状具体为：constant (200-200-200), increasing (100- 200-300), decreasing (300-200-100), and diamond (150-300- 150).<br>如下图所示，constant型效果更好。这点比较有意思，因为在Wide &amp; Deep Model中，采用的是decreasing型。网络结构的效果也取决于实验数据本身。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/20467-5250eab57ad3ab1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="网络结构"></p>
<h1 id="5-DeepFM的实现"><a href="#5-DeepFM的实现" class="headerlink" title="5. DeepFM的实现"></a>5. DeepFM的实现</h1><p>DeepCTR[6]是一个实现了多种深度CTR预估模型的python库，下面引用它基于criteo数据，所实现的DeepFM样例代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from sklearn.metrics import log_loss, roc_auc_score</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import LabelEncoder, MinMaxScaler</span><br><span class="line"></span><br><span class="line">from deepctr.models import DeepFM</span><br><span class="line">from deepctr.utils import SingleFeat</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    data = pd.read_csv(&apos;./criteo_sample.txt&apos;)</span><br><span class="line"></span><br><span class="line">    #拆分稀疏和稠密特征</span><br><span class="line">    sparse_features = [&apos;C&apos; + str(i) for i in range(1, 27)]</span><br><span class="line">    dense_features = [&apos;I&apos; + str(i) for i in range(1, 14)]</span><br><span class="line"></span><br><span class="line">    data[sparse_features] = data[sparse_features].fillna(&apos;-1&apos;, )</span><br><span class="line">    data[dense_features] = data[dense_features].fillna(0, )</span><br><span class="line">    target = [&apos;label&apos;]</span><br><span class="line"></span><br><span class="line">    # 1.类别特征的编码与稠密特征做归一化</span><br><span class="line">    for feat in sparse_features:</span><br><span class="line">        lbe = LabelEncoder()</span><br><span class="line">        data[feat] = lbe.fit_transform(data[feat])</span><br><span class="line">    mms = MinMaxScaler(feature_range=(0, 1))</span><br><span class="line">    data[dense_features] = mms.fit_transform(data[dense_features])</span><br><span class="line"></span><br><span class="line">    # 2.统计稀疏特征类别特征个数，记录稠密特征类目</span><br><span class="line">    sparse_feature_list = [SingleFeat(feat, data[feat].nunique())</span><br><span class="line">                           for feat in sparse_features]</span><br><span class="line">    dense_feature_list = [SingleFeat(feat, 0,)</span><br><span class="line">                          for feat in dense_features]</span><br><span class="line"></span><br><span class="line">    # 3.生成模型输入特征</span><br><span class="line"></span><br><span class="line">    train, test = train_test_split(data, test_size=0.2)</span><br><span class="line">    train_model_input = [train[feat.name].values for feat in sparse_feature_list] + \</span><br><span class="line">                        [train[feat.name].values for feat in dense_feature_list]</span><br><span class="line">    test_model_input = [test[feat.name].values for feat in sparse_feature_list] + \</span><br><span class="line">                       [test[feat.name].values for feat in dense_feature_list]</span><br><span class="line"></span><br><span class="line">    # 4.定义模型、预测、评估模型</span><br><span class="line">    model = DeepFM(&#123;&quot;sparse&quot;: sparse_feature_list,</span><br><span class="line">                    &quot;dense&quot;: dense_feature_list&#125;, task=&apos;binary&apos;)</span><br><span class="line">    model.compile(&quot;adam&quot;, &quot;binary_crossentropy&quot;,</span><br><span class="line">                  metrics=[&apos;binary_crossentropy&apos;], )</span><br><span class="line"></span><br><span class="line">    history = model.fit(train_model_input, train[target].values,</span><br><span class="line">                        batch_size=256, epochs=10, verbose=2, validation_split=0.2, )</span><br><span class="line">    pred_ans = model.predict(test_model_input, batch_size=256)</span><br><span class="line">    print(&quot;test LogLoss&quot;, round(log_loss(test[target].values, pred_ans), 4))</span><br><span class="line">    print(&quot;test AUC&quot;, round(roc_auc_score(test[target].values, pred_ans), 4))</span><br></pre></td></tr></table></figure>

<h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>[1] Guo, Huifeng, et al. “DeepFM: a factorization-machine based neural network for CTR prediction.” arXiv preprint arXiv:1703.04247 (2017).<br>[2] Cheng, Heng-Tze, et al. “Wide &amp; deep learning for recommender systems.” Proceedings of the 1st workshop on deep learning for recommender systems. ACM, 2016.<br>[3] Zhang, Weinan, Tianming Du, and Jun Wang. “Deep learning over multi-field categorical data.” European conference on information retrieval. Springer, Cham, 2016.<br>[4] Rendle, Steffen. “Factorization machines.” 2010 IEEE International Conference on Data Mining. IEEE, 2010.<br>[5] McMahan, H. Brendan. “Follow-the-regularized-leader and mirror descent: Equivalence theorems and l1 regularization.” (2011).<br>[6] DeepCTR  <a href="https://github.com/shenweichen/DeepCTR" target="_blank" rel="noopener">https://github.com/shenweichen/DeepCTR</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/24/DeepFM理论与其应用/" data-id="cjxdyu82800002owfl6gsucm5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/recommendation-system/">recommendation system</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-胡椒全球史读书笔记" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/22/胡椒全球史读书笔记/" class="article-date">
  <time datetime="2019-06-22T09:04:51.000Z" itemprop="datePublished">2019-06-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/22/胡椒全球史读书笔记/">胡椒全球史读书笔记</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>达伽马的光荣与梦想，</p>
<p>胡椒的地理位置，大航海时代的起源。金钱的欲望，伊甸园与约翰王的传说</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/22/胡椒全球史读书笔记/" data-id="cjxdyu82o000g2owfw70j609n" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/history/">history</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-我眼中的云计算" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/22/我眼中的云计算/" class="article-date">
  <time datetime="2019-06-22T07:44:28.000Z" itemprop="datePublished">2019-06-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/22/我眼中的云计算/">我眼中的云计算</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>很多人认为云计算是谷歌和亚马逊最早在2006年使用“云计算”这个词，来描述<br>用户按需使用在互联网上使用软件、接入算力、存储文件这个全新的范式。<br>但是 MIT Technology Review  却将云计算术语的起源，追溯到1996年康柏的一个小组的技术人员描绘了互联网商业前景，并将其称之为云计算。<br>他们的愿景详细而且准确。不只是所有商业软件会迁移上网，而且他们命名了“基于云计算的应用” “cloud computing-enabled applications”比如用户文件存储会变得很普遍。<br>在现代背景下首次使用“云计算”发生在2006年8月9日，当时谷歌CEO埃里克施密特将这一术语引入了行业会议。 “现在有趣的是，有一种新兴的新模式，”施密特说，“我认为人们并不真正了解这个机会究竟有多大。它的前提是数据服务和体系结构应该在服务器上。我们称之为云计算 - 他们应该在某个地方“云”。“<br>在亚马逊，微软和IBM等公司开始推动云计算工作之后，该术语在第二年开始得到更广泛的使用。这也是它首次出现在报纸文章中，例如2007年11月15日的纽约时报报道，其标题为“I.B.M.推动“云计算”，“利用远方的数据。”它描述了“基于互联网的超级计算”的模糊计划。<br>时至今日，云计算已经是家喻户晓，它应该是当今最火热的一个技术名词了。在世界最大的搜索引擎谷歌上搜索”cloud computing”，可以显示有3亿8千万个结果，搜索中文“云计算”也有5500万个结果。</p>
<h1 id="1-风起"><a href="#1-风起" class="headerlink" title="1. 风起"></a>1. 风起</h1><h2 id="Buzzword"><a href="#Buzzword" class="headerlink" title="Buzzword"></a>Buzzword</h2><ul>
<li>云计算也已经成为无处不在的行话，烦人但无法避免。</li>
<li>云是互联网的隐喻，也是互联网的品牌重塑。这就是为什么会有激烈争论的原因。 由于是一个隐喻，它可以接受不同的解释。当然它也值得去花钱。</li>
<li>亚马逊SOA </li>
<li>GAE的崛起</li>
<li>学术界的云</li>
<li>什么做的云</li>
</ul>
<h1 id="2-云涌"><a href="#2-云涌" class="headerlink" title="2.云涌"></a>2.云涌</h1><ul>
<li>微软的逆袭</li>
<li>阿里的宝藏</li>
<li>反对者和落败者</li>
</ul>
<h1 id="3-心动"><a href="#3-心动" class="headerlink" title="3. 心动"></a>3. 心动</h1><ul>
<li>腾讯的追赶</li>
<li>百度的转向</li>
<li>人工智能与云计算</li>
</ul>
<h1 id="4-志未移"><a href="#4-志未移" class="headerlink" title="4. 志未移"></a>4. 志未移</h1><ul>
<li>新玩家</li>
<li>云的下半场</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/06/22/我眼中的云计算/" data-id="cjxdyu82i00052owfx7jgw3dx" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/cloud-computing/">cloud computing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/history/">history</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/big-data/">big data</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cloud-computing/">cloud computing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/history/">history</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ml/">ml</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/recommendation-system/">recommendation system</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/reinforcement-learning/">reinforcement learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/big-data/" style="font-size: 15px;">big data</a> <a href="/tags/cloud-computing/" style="font-size: 10px;">cloud computing</a> <a href="/tags/history/" style="font-size: 15px;">history</a> <a href="/tags/ml/" style="font-size: 10px;">ml</a> <a href="/tags/recommendation-system/" style="font-size: 20px;">recommendation system</a> <a href="/tags/reinforcement-learning/" style="font-size: 10px;">reinforcement learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/06/27/用户行为的深度追踪——事件与埋点/">用户行为的深度追踪——事件与埋点</a>
          </li>
        
          <li>
            <a href="/2019/06/24/糖豆推荐系统第一期开发与评估报告/">糖豆推荐系统第一期开发与评估报告</a>
          </li>
        
          <li>
            <a href="/2019/06/24/糖豆数据仓库模型/">糖豆数据仓库模型</a>
          </li>
        
          <li>
            <a href="/2019/06/24/单步强化学习在糖豆推荐系统的应用/">单步强化学习在糖豆推荐系统的应用</a>
          </li>
        
          <li>
            <a href="/2019/06/24/糖豆实时推荐系统设计与实现/">糖豆实时推荐系统设计与实现</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 ventlam<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>
{"meta":{"version":1,"warehouse":"5.0.0"},"models":{"Asset":[{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.css","path":"fancybox/jquery.fancybox.min.css","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.js","path":"fancybox/jquery.fancybox.min.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/js/jquery-3.6.4.min.js","path":"js/jquery-3.6.4.min.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/js/script.js","path":"js/script.js","modified":0,"renderable":1},{"_id":"node_modules/hexo-theme-landscape/source/css/images/banner.jpg","path":"css/images/banner.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1703420450316},{"_id":"node_modules/hexo-theme-landscape/LICENSE","hash":"c480fce396b23997ee23cc535518ffaaf7f458f8","modified":1703420487491},{"_id":"node_modules/hexo-theme-landscape/README.md","hash":"1a9b279e6dd29fd19245f913f0c4a316ffaa62db","modified":1703420487825},{"_id":"node_modules/hexo-theme-landscape/_config.yml","hash":"b608c1f1322760dce9805285a602a95832730a2e","modified":1703420487836},{"_id":"node_modules/hexo-theme-landscape/package.json","hash":"4bf95d52f77edf811f23f6d264a7493311a8d078","modified":1703420487825},{"_id":"node_modules/hexo-theme-landscape/languages/de-DE.yml","hash":"d29d1c4256b7ed9df42f511c2ff0a23ad5fd6c1f","modified":1703420487836},{"_id":"node_modules/hexo-theme-landscape/languages/de.yml","hash":"3ebf0775abbee928c8d7bda943c191d166ded0d3","modified":1703420487837},{"_id":"node_modules/hexo-theme-landscape/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1703420487837},{"_id":"node_modules/hexo-theme-landscape/languages/en-GB.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1703420487837},{"_id":"node_modules/hexo-theme-landscape/languages/en-US.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1703420487838},{"_id":"node_modules/hexo-theme-landscape/languages/en.yml","hash":"3083f319b352d21d80fc5e20113ddf27889c9d11","modified":1703420487838},{"_id":"node_modules/hexo-theme-landscape/languages/es-ES.yml","hash":"7008a8fc91f18d2a735864817b8ebda30c7a2c66","modified":1703420487839},{"_id":"node_modules/hexo-theme-landscape/languages/es.yml","hash":"76edb1171b86532ef12cfd15f5f2c1ac3949f061","modified":1703420487839},{"_id":"node_modules/hexo-theme-landscape/languages/fr-FR.yml","hash":"8d09dbdab00a30a2870b56f7c0a7ca7deafa7b88","modified":1703420487839},{"_id":"node_modules/hexo-theme-landscape/languages/fr.yml","hash":"415e1c580ced8e4ce20b3b0aeedc3610341c76fb","modified":1703420487839},{"_id":"node_modules/hexo-theme-landscape/languages/hu-HU.yml","hash":"712d18664898fa21ba38d4973e90ef41a324ea25","modified":1703420487840},{"_id":"node_modules/hexo-theme-landscape/languages/hu.yml","hash":"284d557130bf54a74e7dcef9d42096130e4d9550","modified":1703420487840},{"_id":"node_modules/hexo-theme-landscape/languages/it-IT.yml","hash":"2cb6dc2fab9bd2dbe1c8bb869a9e8bf85a564fdd","modified":1703420487841},{"_id":"node_modules/hexo-theme-landscape/languages/it.yml","hash":"89b7d91306b2c1a0f3ac023b657bf974f798a1e8","modified":1703420487841},{"_id":"node_modules/hexo-theme-landscape/languages/ja.yml","hash":"a73e1b9c80fd6e930e2628b393bfe3fb716a21a9","modified":1703420487841},{"_id":"node_modules/hexo-theme-landscape/languages/ja-JP.yml","hash":"08481267e0c112e1f6855620f2837ec4c4a98bbd","modified":1703420487841},{"_id":"node_modules/hexo-theme-landscape/languages/ko-KR.yml","hash":"19209ad8f9d4057e8df808937f950eb265e1db69","modified":1703420487842},{"_id":"node_modules/hexo-theme-landscape/languages/ko.yml","hash":"881d6a0a101706e0452af81c580218e0bfddd9cf","modified":1703420487842},{"_id":"node_modules/hexo-theme-landscape/languages/mn-MN.yml","hash":"b9e5f3e7c0c2f779cf2cfded6db847b5941637ca","modified":1703420487842},{"_id":"node_modules/hexo-theme-landscape/languages/mn.yml","hash":"2e7523951072a9403ead3840ad823edd1084c116","modified":1703420487842},{"_id":"node_modules/hexo-theme-landscape/languages/nl-NL.yml","hash":"5ebbc30021f05d99938f96dfff280392df7f91f0","modified":1703420487843},{"_id":"node_modules/hexo-theme-landscape/languages/nl.yml","hash":"12ed59faba1fc4e8cdd1d42ab55ef518dde8039c","modified":1703420487843},{"_id":"node_modules/hexo-theme-landscape/languages/no.yml","hash":"965a171e70347215ec726952e63f5b47930931ef","modified":1703420487843},{"_id":"node_modules/hexo-theme-landscape/languages/pt-PT.yml","hash":"0f852b6b228e6ea59aa3540574bb89b233f2a098","modified":1703420487843},{"_id":"node_modules/hexo-theme-landscape/languages/pt.yml","hash":"57d07b75d434fbfc33b0ddb543021cb5f53318a8","modified":1703420487844},{"_id":"node_modules/hexo-theme-landscape/languages/ru-RU.yml","hash":"360d11a28bb768afb1dd15f63fa7fd3a8cc547ee","modified":1703420487844},{"_id":"node_modules/hexo-theme-landscape/languages/ru.yml","hash":"4fda301bbd8b39f2c714e2c934eccc4b27c0a2b0","modified":1703420487845},{"_id":"node_modules/hexo-theme-landscape/languages/th-TH.yml","hash":"ebfdba9bc4842c829473c1e6e4544344f182724d","modified":1703420487845},{"_id":"node_modules/hexo-theme-landscape/languages/th.yml","hash":"84a55b00aa01f03982be294e43c33a20e6d32862","modified":1703420487846},{"_id":"node_modules/hexo-theme-landscape/languages/tr.yml","hash":"a1cdbfa17682d7a971de8ab8588bf57c74224b5b","modified":1703420487847},{"_id":"node_modules/hexo-theme-landscape/languages/zh-CN.yml","hash":"1efd95774f401c80193eac6ee3f1794bfe93dc5a","modified":1703420487847},{"_id":"node_modules/hexo-theme-landscape/languages/zh-TW.yml","hash":"53ce3000c5f767759c7d2c4efcaa9049788599c3","modified":1703420487848},{"_id":"node_modules/hexo-theme-landscape/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1703420487687},{"_id":"node_modules/hexo-theme-landscape/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1703420487670},{"_id":"node_modules/hexo-theme-landscape/layout/layout.ejs","hash":"0d1765036e4874500e68256fedb7470e96eeb6ee","modified":1703420487720},{"_id":"node_modules/hexo-theme-landscape/layout/index.ejs","hash":"aa1b4456907bdb43e629be3931547e2d29ac58c8","modified":1703420487717},{"_id":"node_modules/hexo-theme-landscape/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1703420487727},{"_id":"node_modules/hexo-theme-landscape/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1703420487728},{"_id":"node_modules/hexo-theme-landscape/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1703420487735},{"_id":"node_modules/hexo-theme-landscape/scripts/fancybox.js","hash":"c857d7a5e4a5d71c743a009c5932bf84229db428","modified":1703420487803},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/after-footer.ejs","hash":"377d257d5d16e0158a4405c72401517b074fd7ff","modified":1703420487596},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/archive-post.ejs","hash":"c7a71425a946d05414c069ec91811b5c09a92c47","modified":1703420487605},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/archive.ejs","hash":"7cb70a7a54f8c7ae49b10d1f37c0a9b74eab8826","modified":1703420487620},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/article.ejs","hash":"56597e951203dd662a6d2c817c7c4f1c920d4a25","modified":1703420487674},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/footer.ejs","hash":"3656eb692254346671abc03cb3ba1459829e0dce","modified":1703420487692},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/gauges-analytics.ejs","hash":"21a1e2a3907d1a3dad1cd0ab855fe6735f233c74","modified":1703420487697},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/google-analytics.ejs","hash":"2ea7442ea1e1a8ab4e41e26c563f58413b59a3d0","modified":1703420487700},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/head.ejs","hash":"f05bced793b0314d4f2ef0c993b3a51d0b7d203a","modified":1703420487708},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/header.ejs","hash":"6a5033d189554c9a6d42e2ef7952ae5c9742648e","modified":1703420487715},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/mobile-nav.ejs","hash":"e952a532dfc583930a666b9d4479c32d4a84b44e","modified":1703420487724},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/sidebar.ejs","hash":"930da35cc2d447a92e5ee8f835735e6fd2232469","modified":1703420487730},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/archive.ejs","hash":"beb4a86fcc82a9bdda9289b59db5a1988918bec3","modified":1703420487653},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/category.ejs","hash":"dd1e5af3c6af3f5d6c85dfd5ca1766faed6a0b05","modified":1703420487684},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/recent_posts.ejs","hash":"60c4b012dcc656438ff59997e60367e5a21ab746","modified":1703420487729},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/tag.ejs","hash":"2de380865df9ab5f577f7d3bcadf44261eb5faae","modified":1703420487733},{"_id":"node_modules/hexo-theme-landscape/layout/_widget/tagcloud.ejs","hash":"b4a2079101643f63993dcdb32925c9b071763b46","modified":1703420487736},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.css","hash":"1be9b79be02a1cfc5d96c4a5e0feb8f472babd95","modified":1703420487576},{"_id":"node_modules/hexo-theme-landscape/source/css/_extend.styl","hash":"222fbe6d222531d61c1ef0f868c90f747b1c2ced","modified":1703420487826},{"_id":"node_modules/hexo-theme-landscape/source/css/_variables.styl","hash":"ca28281423ae57d76b6c1eb91cd845fd4e518bd6","modified":1703420487827},{"_id":"node_modules/hexo-theme-landscape/source/css/style.styl","hash":"e55a1d92954ed20f6887f92dc727bb995a010a43","modified":1703420487835},{"_id":"node_modules/hexo-theme-landscape/source/js/script.js","hash":"49773efcb2221bbdf2d86f3f5c5ff2d841b528cc","modified":1703420487823},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/category.ejs","hash":"c6bcd0e04271ffca81da25bcff5adf3d46f02fc0","modified":1703420487681},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/date.ejs","hash":"f1458584b679545830b75bef2526e2f3eb931045","modified":1703420487690},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/gallery.ejs","hash":"3d9d81a3c693ff2378ef06ddb6810254e509de5b","modified":1703420487694},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/tag.ejs","hash":"2fcb0bf9c8847a644167a27824c9bb19ac74dd14","modified":1703420487732},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/nav.ejs","hash":"16a904de7bceccbb36b4267565f2215704db2880","modified":1703420487726},{"_id":"node_modules/hexo-theme-landscape/layout/_partial/post/title.ejs","hash":"4d7e62574ddf46de9b41605fe3140d77b5ddb26d","modified":1703420487737},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/archive.styl","hash":"db15f5677dc68f1730e82190bab69c24611ca292","modified":1703420487829},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/article.styl","hash":"2d1f6f79ebf9cb55ebdb3865a2474437eb2b37c6","modified":1703420487829},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/comment.styl","hash":"79d280d8d203abb3bd933ca9b8e38c78ec684987","modified":1703420487830},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/footer.styl","hash":"e35a060b8512031048919709a8e7b1ec0e40bc1b","modified":1703420487830},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/header.styl","hash":"268d2989acb06e2ddd06cc36a6918c6cd865476b","modified":1703420487832},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/highlight.styl","hash":"9cc3b2927d814f2f6e8e188f9d3657b94f4c6ef3","modified":1703420487833},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/mobile.styl","hash":"a399cf9e1e1cec3e4269066e2948d7ae5854d745","modified":1703420487834},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/sidebar-aside.styl","hash":"890349df5145abf46ce7712010c89237900b3713","modified":1703420487834},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/sidebar-bottom.styl","hash":"8fd4f30d319542babfd31f087ddbac550f000a8a","modified":1703420487834},{"_id":"node_modules/hexo-theme-landscape/source/css/_partial/sidebar.styl","hash":"404ec059dc674a48b9ab89cd83f258dec4dcb24d","modified":1703420487835},{"_id":"node_modules/hexo-theme-landscape/source/css/_util/grid.styl","hash":"0bf55ee5d09f193e249083602ac5fcdb1e571aed","modified":1703420487832},{"_id":"node_modules/hexo-theme-landscape/source/css/_util/mixin.styl","hash":"44f32767d9fd3c1c08a60d91f181ee53c8f0dbb3","modified":1703420487833},{"_id":"node_modules/hexo-theme-landscape/source/fancybox/jquery.fancybox.min.js","hash":"6181412e73966696d08e1e5b1243a572d0f22ba6","modified":1703420487822},{"_id":"node_modules/hexo-theme-landscape/source/js/jquery-3.6.4.min.js","hash":"eda46747c71d38a880bee44f9a439c3858bb8f99","modified":1703420487810},{"_id":"node_modules/hexo-theme-landscape/source/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1703420487797},{"_id":"public/2023/12/24/hello-world/index.html","hash":"f0d534159cba018fece328ed4ad09fa23d6f8f17","modified":1703422305638},{"_id":"public/archives/index.html","hash":"aaa51f477ae6d2b58e145a9513d03b4767df04f4","modified":1703422305638},{"_id":"public/archives/2023/index.html","hash":"b9b3378b16a446df5a48e55d6159a627bfae844f","modified":1703422305638},{"_id":"public/archives/2023/12/index.html","hash":"2cd269b4995f2c7744acfe5e0b8aeea0b43b91d0","modified":1703422305638},{"_id":"public/index.html","hash":"7c7c2acb4686e9e2fd287f9121d5291afb62bc0e","modified":1703422305638},{"_id":"public/fancybox/jquery.fancybox.min.css","hash":"1be9b79be02a1cfc5d96c4a5e0feb8f472babd95","modified":1703422305638},{"_id":"public/fancybox/jquery.fancybox.min.js","hash":"6181412e73966696d08e1e5b1243a572d0f22ba6","modified":1703422305638},{"_id":"public/css/style.css","hash":"ddb3792605d744ab3d9f0a649c82b62e9b16daa6","modified":1703422305638},{"_id":"public/js/script.js","hash":"49773efcb2221bbdf2d86f3f5c5ff2d841b528cc","modified":1703422305638},{"_id":"public/js/jquery-3.6.4.min.js","hash":"eda46747c71d38a880bee44f9a439c3858bb8f99","modified":1703422305638},{"_id":"public/css/images/banner.jpg","hash":"f44aa591089fcb3ec79770a1e102fd3289a7c6a6","modified":1703422305638},{"_id":"source/.DS_Store","hash":"b48c4f7d61a5928be717d4bd654481ff1eab36ee","modified":1703421252220},{"_id":"source/_posts/从零开始打造更强的私有GPT大模型- RAG教程01.md","hash":"edadebd9680beac83755af658fa67e97106c598a","modified":1703421466465},{"_id":"public/2023/12/24/从零开始打造更强的私有GPT大模型- RAG教程01/index.html","hash":"0bebc3977c06d81b3e6dde7bd05b87da38ea1168","modified":1703422305638},{"_id":"public/tags/vector/index.html","hash":"04c4d078c20aeef6c4d21a90aad79f2dc769b53c","modified":1703422305638},{"_id":"public/tags/LLM/index.html","hash":"36667335e52f4059aad9a647255f095698c585a1","modified":1703422305638},{"_id":"public/tags/RAG/index.html","hash":"8db01e208d440fdf9a4ef9680557145c54e1379a","modified":1703422305638},{"_id":"public/tags/embeeding/index.html","hash":"c73dff952faa07f607f40dc4dcfc58969ea6be29","modified":1703422305638},{"_id":"public/categories/AI/index.html","hash":"d26ab3a6b25c68d7afa70754359aaf4da79c76f7","modified":1703422305638}],"Category":[{"name":"AI","_id":"clqjhnr990001cu9phgy21k6e"}],"Data":[],"Page":[],"Post":[{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2023-12-24T12:20:50.316Z","updated":"2023-12-24T12:20:50.316Z","comments":1,"layout":"post","photos":[],"_id":"clqjgrpbv0000a09pddvee4tc","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"从零开始打造更强的私有GPT大模型- RAG教程01","_content":"![vent1924_convey_a_sense_of_cutting-edge_technology_and_innovati_58510eb0-17a4-4f8a-baa3-4c159386c609.png](https://s.alidraft.com/vent/vent1924_convey_a_sense_of_cutting-edge_technology_and_innovati_58510eb0-17a4-4f8a-baa3-4c159386c609.png)\n\n## RAG理论\n### 1. 什么是RAG\n众所周知，大模型基于海量的数据来训练，它具备非常强大的智能，能够回答各种问题。但是我们在使用过程中发现，通用大模型在某些专业领域能力还不够强，很多**领域相关问题很难回答得上来**。通常，预训练（pre-train）的大模型只懂得它训练时用的数据，对于训练集之外的新信息（比如网络搜索新数据或特定行业的知识）就不太清楚。\n\n那么怎么构建一个私有的GPT大模型呢？方法有很多种，包括 1. 重新训练**私有领域数据**的大模型，2. 基于已有大模型做专有数据的**微调**(FineTuning) 3. 通过RAG技术，优化大模型基础能力。4. 通过Prompt 工程把私有数据在对话中给到大模型。\n\n**RAG**: Retrieval Augmented Generation，检索增强生成技术。RAG由FaceBook AI实验室 于2020年提出，他们的论文[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)， 提供了一种通过给大模型提供向量数据来增强模型能力的方法。\n\n快速来看一下文章摘要：\n\n> 1. **背景与挑战**：这些大型预训练语言模型，因其存储了大量事实知识和在 NLP 任务中的出色表现而闻名。但它们在精确获取和处理知识方面存在局限，尤其是在知识要求高的任务中。这导致它们在特定任务的架构上表现不佳。此外，如何提供决策依据和更新模型中的知识仍是一个挑战。\n    \n> 2. **检索增强生成（RAG）方法**：文章提出了一种 RAG 模型的微调方案，这些模型巧妙地结合了预训练的参数记忆（例如 seq2seq 模型）和非参数记忆（例如维基百科的密集向量索引）。通过预训练的神经检索器，这种组合被赋予了新的生命。RAG 模型的目标是利用这两种记忆类型，使语言生成更加生动和具有创造力。\n    \n>3. **两种RAG形式**：研究精心比较了两种 RAG 形式。一种在整个生成过程中使用相同的检索段落，另一种则可以为每个词汇使用不同的段落。。\n    \n>4. **性能与评估**：在多种知识密集型 NLP 任务中，RAG 模型经过微调和评估后，创造了三个开放领域问答任务的新纪录，超越了传统的参数型 seq2seq 模型和专门的检索-提取架构。在语言生成方面，与仅使用参数的 seq2seq 模型相比，RAG 模型能生成更加具体、多样和真实的语言。\n\n\n整体看一下：论文对RAG技术的理解。 主要由 **Retriever**和**Generator**两大部分组成。\n![image.png](https://s.zhangguiyi.cn/vent/202312181613600.png)\n\n随着时间的推移，尤其是大模型的进步，RAG的架构有些变化，更加组件化和清晰化。\n如果希望了解更多关于RAG的历史和发展，推荐仔细阅读综述文章： **[Retrieval-Augmented Generation for Large Language Models: A Survey\"](https://arxiv.org/pdf/2312.10997.pdf)** \n\n![image.png](https://s.alidraft.com/vent/202312211120837.png)\n\n### 2. 进一步理解RAG\n\n第一节相对学术化一些，我们来看一个更好理解的图：\n\n![](https://docs.llamaindex.ai/en/stable/_images/basic_rag.png)\n\n从上图可以看到**RAG**的系统核心,由User(Query),(Vector) Index , LLM 三大组件组成。\n#### 2.1 三大组件\n\n1. **用户发起的查询(User->query)**。这种查询一般都是自然语言的，用户不再需要学习类似于之前搜索引擎的DSL或者数据库的SQL。这样大幅降低这类系统的使用门槛。\n2. **模型所需的外部数据(Index -> prompt)**。可以看到，RAG系统的核心工作其实在这个组件。\n\t\t1. **索引**：将不同类似的用户数据，比如结构化的关系数据库、非结构化的文本、甚至是可编程的API，通过向量嵌入(Vector embedding)方法来将它们变成向量数据。更多关于向量数据库的文章，可以访问我之前的博客。涉及到**向量化(Vector embedding)方法**和**切片(Chunk)方法**。\n\t\t2. **召回**，向量数据的召回，本质上通过余弦相似度来找到最匹配的多个向量。目的是从大量数据中快速筛选出与查询最相关的文档子集，为后续的更详细检索过程提供一个更专注的候选集。这种方法旨在提高检索过程的效率和效果，减少计算资源的需求，并加速响应时间。\n\t\t3. **查询**。这里查询方法有很多种，涉及到向量数据库的相关度计算与评估，不同的查询策略。\n3. **大模型(LLM)**。这里面的大模型可以是开源的Llama2/Mistral 等，也可以是闭源的GPT3.5/4等。\n\n\n#### 2.2 实现RAG的五个步骤\n重复总结一下，实现RAG中有五个关键步骤，如下图所示：\n![](https://docs.llamaindex.ai/en/stable/_images/stages.png)\n\n- **加载**：指将各种文本文件、PDF、其他网站、数据库还是API等数据，导入我们工作流的步骤。\n- **索引**：和普通关系数据库无本质差异，在于通过索引加速查询。不同的是，具体的索引算法。\n- **存储**：存储索引以及其他元数据，以避免重新索引\n- **查询**：对于任何给定的索引，可以进行多种查询，包括子查询、多步查询和混合策略。\n- **评估**：提供了关于查询的响应有多准确、快速的客观衡量。\n\n\n### 3. RAG的进化\n![image.png](https://s.alidraft.com/vent/202312211123188.png)\n\n1. **初级RAG（Naive RAG）**：这是RAG的最初形式，包括基本的索引、检索和生成过程。它以简单的方式将检索到的信息与生成任务相结合，但可能存在准确性和效率的问题。\n    \n2. **高级RAG（Advanced RAG）**：在初级RAG的基础上，高级RAG引入了预检索和后检索处理方法，优化了索引和检索流程。这种范式致力于提高检索内容的质量和相关性，以及提升生成任务的效果。\n    \n3. **模块化RAG（Modular RAG）**：这种范式通过引入多样的模块，如搜索模块、记忆模块和额外的生成模块，提供了更多的灵活性和定制化能力。模块化RAG允许根据特定问题的上下文重新配置或替换模块，实现更复杂和高效的检索增强生成任务。\n\n再次推荐阅读综述文章： **[Retrieval-Augmented Generation for Large Language Models: A Survey\"](https://arxiv.org/pdf/2312.10997.pdf)** \n### 4. RAG能干嘛？\nRAG的LLM应用的用例无穷无尽，但大致可以分为三类：\n\n[**查询引擎**](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html): 查询引擎允许您对您的数据提出问题。它接收自然语言查询，并返回响应，以及检索并传递给LLM的参考上下文。\n\n[**聊天引擎**](https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html)： 聊天引擎用于与您的数据进行**多轮对话**。\n\n[**Agent(代理)**](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html): Agent由LLM驱动，能够实现自动决策。可以采取任意数量的步骤来完成给定的任务，动态地决定最佳行动方案。Agent某种意义上来讲是一种AGI。\n### 5. 给我也搞一个\n**可以！**\n接下来我们基于[Llama Index](https://www.llamaindex.ai/)库来实现一个网页数据的Q&A机器人。\n\n## RAG实战\n2023 年以来，出现了大量的开源 & 闭源LLM大模型，基本上都能够在上面构建 RAG 系统。\n最常见的方式包括： \n- GPT-3.5/4 + RAG（闭源方案） \n- Llama 2 / Mistral + RAG（开源方案）\n### 基于LLama-Index 和 GPT3.5 来构建\n\n我们基于来LLama-Index 和 GPT3.5 来构建一个RAG系统，它能够访问你指定的**网页数据**，你可以提问关于这个网页的**任何内容**。\n\n> 1. Llama-Index是一个简单灵活的数据框架，用于连接自定义数据源到大型语言模型（LLM）。\n> 2. 它提供了API和入门教程，方便用户进行数据的摄取和查询。\n> 3. Llama-Index可以作为桥梁，连接自定义数据和大型语言模型。\n> 4. 通过Llama-Index，用户可以轻松构建应用程序，并访问私有或特定领域的数据。\n\n* 复习一遍流程：加载、索引、存储、查询、评估\n### 加载库和数据\n```sh\n# 安装所需的库\n!pip install llama-index transformers\n\n```\n\n```python\nfrom llama_index.readers import BeautifulSoupWebReader\n\n  \n# 访问智写AI的官网博客\nurl = \"https://www.draftai.cn/2023/12/19/chatonce-support-chat-with-file/\"\n# 通过BeautifulSoupWebReader 来加载数据\ndocuments = BeautifulSoupWebReader().load_data([url])\n\n```\n\n### 索引&存储\n```python\n\n## index \nimport os\n\nimport openai\n\n#设置你在openai的密钥\nos.environ['OPENAI_API_KEY'] = \"sk-\"\n\nopenai.api_key = os.environ['OPENAI_API_KEY']\nfrom llama_index.llms import OpenAI\n\n  \n## 指定GPT3.5模型，记得要用gpt-3.5-turbo-1106，更便宜\nllm = OpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)\nfrom llama_index import ServiceContext\n\n  \n## 向量化，采用BAAI的向量库，开源免费，比用OpenAI的embbeding便宜。\nservice_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-small-zh-v1.5\") #BAAI/bge-small-zh-v1.5. BAAI/bge-small-en-v1.5\n\n```\n### 查询\n```python\nfrom llama_index.response.notebook_utils import display_response\nimport logging\n\nimport sys\n\n  \n# 打印日志组件\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\nquery_engine = vector_index.as_query_engine(response_mode=\"compact\")\n\n  \n# 简单查询问题\nresponse = query_engine.query(\"智写AI能干嘛?它最新的功能是什么？\")\n\n  \n# 展示返回结果\ndisplay_response(response)\n```\n\n### 不同的查询策略和效果\n![image.png](https://s.alidraft.com/vent/202312211901720.png)\n\n\n![image.png](https://s.alidraft.com/vent/202312211900917.png)\n\n\n![image.png](https://s.alidraft.com/vent/202312211903829.png)\n\n### 完整代码\n\n可以复制我在Colab的[notebook ](https://colab.research.google.com/drive/1gvqOOpxduIKS3EPCwG3mkn8hnEmWED1L?usp=sharing)直接运行。\n\n## 参考资料\n  \n[1]Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks: _https://arxiv.org/abs/2005.11401_\n\n[2]**查询引擎**: _https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html_\n\n[3]**聊天引擎**: _https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html_\n\n[4]**Agent(代理)**: _https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html_\n\n[5]Llama Index: _https://www.llamaindex.ai/_\n\n[6]notebook : _https://colab.research.google.com/drive/1gvqOOpxduIKS3EPCwG3mkn8hnEmWED1L?usp=sharing_\n","source":"_posts/从零开始打造更强的私有GPT大模型- RAG教程01.md","raw":"---\ntitle: 从零开始打造更强的私有GPT大模型- RAG教程01\ncategories: AI\ntags: \n    - vector \n    - LLM\n    - RAG\n    - embeeding\n---\n![vent1924_convey_a_sense_of_cutting-edge_technology_and_innovati_58510eb0-17a4-4f8a-baa3-4c159386c609.png](https://s.alidraft.com/vent/vent1924_convey_a_sense_of_cutting-edge_technology_and_innovati_58510eb0-17a4-4f8a-baa3-4c159386c609.png)\n\n## RAG理论\n### 1. 什么是RAG\n众所周知，大模型基于海量的数据来训练，它具备非常强大的智能，能够回答各种问题。但是我们在使用过程中发现，通用大模型在某些专业领域能力还不够强，很多**领域相关问题很难回答得上来**。通常，预训练（pre-train）的大模型只懂得它训练时用的数据，对于训练集之外的新信息（比如网络搜索新数据或特定行业的知识）就不太清楚。\n\n那么怎么构建一个私有的GPT大模型呢？方法有很多种，包括 1. 重新训练**私有领域数据**的大模型，2. 基于已有大模型做专有数据的**微调**(FineTuning) 3. 通过RAG技术，优化大模型基础能力。4. 通过Prompt 工程把私有数据在对话中给到大模型。\n\n**RAG**: Retrieval Augmented Generation，检索增强生成技术。RAG由FaceBook AI实验室 于2020年提出，他们的论文[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)， 提供了一种通过给大模型提供向量数据来增强模型能力的方法。\n\n快速来看一下文章摘要：\n\n> 1. **背景与挑战**：这些大型预训练语言模型，因其存储了大量事实知识和在 NLP 任务中的出色表现而闻名。但它们在精确获取和处理知识方面存在局限，尤其是在知识要求高的任务中。这导致它们在特定任务的架构上表现不佳。此外，如何提供决策依据和更新模型中的知识仍是一个挑战。\n    \n> 2. **检索增强生成（RAG）方法**：文章提出了一种 RAG 模型的微调方案，这些模型巧妙地结合了预训练的参数记忆（例如 seq2seq 模型）和非参数记忆（例如维基百科的密集向量索引）。通过预训练的神经检索器，这种组合被赋予了新的生命。RAG 模型的目标是利用这两种记忆类型，使语言生成更加生动和具有创造力。\n    \n>3. **两种RAG形式**：研究精心比较了两种 RAG 形式。一种在整个生成过程中使用相同的检索段落，另一种则可以为每个词汇使用不同的段落。。\n    \n>4. **性能与评估**：在多种知识密集型 NLP 任务中，RAG 模型经过微调和评估后，创造了三个开放领域问答任务的新纪录，超越了传统的参数型 seq2seq 模型和专门的检索-提取架构。在语言生成方面，与仅使用参数的 seq2seq 模型相比，RAG 模型能生成更加具体、多样和真实的语言。\n\n\n整体看一下：论文对RAG技术的理解。 主要由 **Retriever**和**Generator**两大部分组成。\n![image.png](https://s.zhangguiyi.cn/vent/202312181613600.png)\n\n随着时间的推移，尤其是大模型的进步，RAG的架构有些变化，更加组件化和清晰化。\n如果希望了解更多关于RAG的历史和发展，推荐仔细阅读综述文章： **[Retrieval-Augmented Generation for Large Language Models: A Survey\"](https://arxiv.org/pdf/2312.10997.pdf)** \n\n![image.png](https://s.alidraft.com/vent/202312211120837.png)\n\n### 2. 进一步理解RAG\n\n第一节相对学术化一些，我们来看一个更好理解的图：\n\n![](https://docs.llamaindex.ai/en/stable/_images/basic_rag.png)\n\n从上图可以看到**RAG**的系统核心,由User(Query),(Vector) Index , LLM 三大组件组成。\n#### 2.1 三大组件\n\n1. **用户发起的查询(User->query)**。这种查询一般都是自然语言的，用户不再需要学习类似于之前搜索引擎的DSL或者数据库的SQL。这样大幅降低这类系统的使用门槛。\n2. **模型所需的外部数据(Index -> prompt)**。可以看到，RAG系统的核心工作其实在这个组件。\n\t\t1. **索引**：将不同类似的用户数据，比如结构化的关系数据库、非结构化的文本、甚至是可编程的API，通过向量嵌入(Vector embedding)方法来将它们变成向量数据。更多关于向量数据库的文章，可以访问我之前的博客。涉及到**向量化(Vector embedding)方法**和**切片(Chunk)方法**。\n\t\t2. **召回**，向量数据的召回，本质上通过余弦相似度来找到最匹配的多个向量。目的是从大量数据中快速筛选出与查询最相关的文档子集，为后续的更详细检索过程提供一个更专注的候选集。这种方法旨在提高检索过程的效率和效果，减少计算资源的需求，并加速响应时间。\n\t\t3. **查询**。这里查询方法有很多种，涉及到向量数据库的相关度计算与评估，不同的查询策略。\n3. **大模型(LLM)**。这里面的大模型可以是开源的Llama2/Mistral 等，也可以是闭源的GPT3.5/4等。\n\n\n#### 2.2 实现RAG的五个步骤\n重复总结一下，实现RAG中有五个关键步骤，如下图所示：\n![](https://docs.llamaindex.ai/en/stable/_images/stages.png)\n\n- **加载**：指将各种文本文件、PDF、其他网站、数据库还是API等数据，导入我们工作流的步骤。\n- **索引**：和普通关系数据库无本质差异，在于通过索引加速查询。不同的是，具体的索引算法。\n- **存储**：存储索引以及其他元数据，以避免重新索引\n- **查询**：对于任何给定的索引，可以进行多种查询，包括子查询、多步查询和混合策略。\n- **评估**：提供了关于查询的响应有多准确、快速的客观衡量。\n\n\n### 3. RAG的进化\n![image.png](https://s.alidraft.com/vent/202312211123188.png)\n\n1. **初级RAG（Naive RAG）**：这是RAG的最初形式，包括基本的索引、检索和生成过程。它以简单的方式将检索到的信息与生成任务相结合，但可能存在准确性和效率的问题。\n    \n2. **高级RAG（Advanced RAG）**：在初级RAG的基础上，高级RAG引入了预检索和后检索处理方法，优化了索引和检索流程。这种范式致力于提高检索内容的质量和相关性，以及提升生成任务的效果。\n    \n3. **模块化RAG（Modular RAG）**：这种范式通过引入多样的模块，如搜索模块、记忆模块和额外的生成模块，提供了更多的灵活性和定制化能力。模块化RAG允许根据特定问题的上下文重新配置或替换模块，实现更复杂和高效的检索增强生成任务。\n\n再次推荐阅读综述文章： **[Retrieval-Augmented Generation for Large Language Models: A Survey\"](https://arxiv.org/pdf/2312.10997.pdf)** \n### 4. RAG能干嘛？\nRAG的LLM应用的用例无穷无尽，但大致可以分为三类：\n\n[**查询引擎**](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html): 查询引擎允许您对您的数据提出问题。它接收自然语言查询，并返回响应，以及检索并传递给LLM的参考上下文。\n\n[**聊天引擎**](https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html)： 聊天引擎用于与您的数据进行**多轮对话**。\n\n[**Agent(代理)**](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html): Agent由LLM驱动，能够实现自动决策。可以采取任意数量的步骤来完成给定的任务，动态地决定最佳行动方案。Agent某种意义上来讲是一种AGI。\n### 5. 给我也搞一个\n**可以！**\n接下来我们基于[Llama Index](https://www.llamaindex.ai/)库来实现一个网页数据的Q&A机器人。\n\n## RAG实战\n2023 年以来，出现了大量的开源 & 闭源LLM大模型，基本上都能够在上面构建 RAG 系统。\n最常见的方式包括： \n- GPT-3.5/4 + RAG（闭源方案） \n- Llama 2 / Mistral + RAG（开源方案）\n### 基于LLama-Index 和 GPT3.5 来构建\n\n我们基于来LLama-Index 和 GPT3.5 来构建一个RAG系统，它能够访问你指定的**网页数据**，你可以提问关于这个网页的**任何内容**。\n\n> 1. Llama-Index是一个简单灵活的数据框架，用于连接自定义数据源到大型语言模型（LLM）。\n> 2. 它提供了API和入门教程，方便用户进行数据的摄取和查询。\n> 3. Llama-Index可以作为桥梁，连接自定义数据和大型语言模型。\n> 4. 通过Llama-Index，用户可以轻松构建应用程序，并访问私有或特定领域的数据。\n\n* 复习一遍流程：加载、索引、存储、查询、评估\n### 加载库和数据\n```sh\n# 安装所需的库\n!pip install llama-index transformers\n\n```\n\n```python\nfrom llama_index.readers import BeautifulSoupWebReader\n\n  \n# 访问智写AI的官网博客\nurl = \"https://www.draftai.cn/2023/12/19/chatonce-support-chat-with-file/\"\n# 通过BeautifulSoupWebReader 来加载数据\ndocuments = BeautifulSoupWebReader().load_data([url])\n\n```\n\n### 索引&存储\n```python\n\n## index \nimport os\n\nimport openai\n\n#设置你在openai的密钥\nos.environ['OPENAI_API_KEY'] = \"sk-\"\n\nopenai.api_key = os.environ['OPENAI_API_KEY']\nfrom llama_index.llms import OpenAI\n\n  \n## 指定GPT3.5模型，记得要用gpt-3.5-turbo-1106，更便宜\nllm = OpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)\nfrom llama_index import ServiceContext\n\n  \n## 向量化，采用BAAI的向量库，开源免费，比用OpenAI的embbeding便宜。\nservice_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-small-zh-v1.5\") #BAAI/bge-small-zh-v1.5. BAAI/bge-small-en-v1.5\n\n```\n### 查询\n```python\nfrom llama_index.response.notebook_utils import display_response\nimport logging\n\nimport sys\n\n  \n# 打印日志组件\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\nlogging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\nquery_engine = vector_index.as_query_engine(response_mode=\"compact\")\n\n  \n# 简单查询问题\nresponse = query_engine.query(\"智写AI能干嘛?它最新的功能是什么？\")\n\n  \n# 展示返回结果\ndisplay_response(response)\n```\n\n### 不同的查询策略和效果\n![image.png](https://s.alidraft.com/vent/202312211901720.png)\n\n\n![image.png](https://s.alidraft.com/vent/202312211900917.png)\n\n\n![image.png](https://s.alidraft.com/vent/202312211903829.png)\n\n### 完整代码\n\n可以复制我在Colab的[notebook ](https://colab.research.google.com/drive/1gvqOOpxduIKS3EPCwG3mkn8hnEmWED1L?usp=sharing)直接运行。\n\n## 参考资料\n  \n[1]Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks: _https://arxiv.org/abs/2005.11401_\n\n[2]**查询引擎**: _https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html_\n\n[3]**聊天引擎**: _https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html_\n\n[4]**Agent(代理)**: _https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html_\n\n[5]Llama Index: _https://www.llamaindex.ai/_\n\n[6]notebook : _https://colab.research.google.com/drive/1gvqOOpxduIKS3EPCwG3mkn8hnEmWED1L?usp=sharing_\n","slug":"从零开始打造更强的私有GPT大模型- RAG教程01","published":1,"date":"2023-12-24T03:56:00.551Z","updated":"2023-12-24T12:37:46.465Z","comments":1,"layout":"post","photos":[],"_id":"clqjhnr950000cu9p2dxx94fx","content":"<p><img src=\"https://s.alidraft.com/vent/vent1924_convey_a_sense_of_cutting-edge_technology_and_innovati_58510eb0-17a4-4f8a-baa3-4c159386c609.png\" alt=\"vent1924_convey_a_sense_of_cutting-edge_technology_and_innovati_58510eb0-17a4-4f8a-baa3-4c159386c609.png\"></p>\n<h2 id=\"RAG理论\"><a href=\"#RAG理论\" class=\"headerlink\" title=\"RAG理论\"></a>RAG理论</h2><h3 id=\"1-什么是RAG\"><a href=\"#1-什么是RAG\" class=\"headerlink\" title=\"1. 什么是RAG\"></a>1. 什么是RAG</h3><p>众所周知，大模型基于海量的数据来训练，它具备非常强大的智能，能够回答各种问题。但是我们在使用过程中发现，通用大模型在某些专业领域能力还不够强，很多<strong>领域相关问题很难回答得上来</strong>。通常，预训练（pre-train）的大模型只懂得它训练时用的数据，对于训练集之外的新信息（比如网络搜索新数据或特定行业的知识）就不太清楚。</p>\n<p>那么怎么构建一个私有的GPT大模型呢？方法有很多种，包括 1. 重新训练<strong>私有领域数据</strong>的大模型，2. 基于已有大模型做专有数据的<strong>微调</strong>(FineTuning) 3. 通过RAG技术，优化大模型基础能力。4. 通过Prompt 工程把私有数据在对话中给到大模型。</p>\n<p><strong>RAG</strong>: Retrieval Augmented Generation，检索增强生成技术。RAG由FaceBook AI实验室 于2020年提出，他们的论文<a href=\"https://arxiv.org/abs/2005.11401\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a>， 提供了一种通过给大模型提供向量数据来增强模型能力的方法。</p>\n<p>快速来看一下文章摘要：</p>\n<blockquote>\n<ol>\n<li><strong>背景与挑战</strong>：这些大型预训练语言模型，因其存储了大量事实知识和在 NLP 任务中的出色表现而闻名。但它们在精确获取和处理知识方面存在局限，尤其是在知识要求高的任务中。这导致它们在特定任务的架构上表现不佳。此外，如何提供决策依据和更新模型中的知识仍是一个挑战。</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"2\">\n<li><strong>检索增强生成（RAG）方法</strong>：文章提出了一种 RAG 模型的微调方案，这些模型巧妙地结合了预训练的参数记忆（例如 seq2seq 模型）和非参数记忆（例如维基百科的密集向量索引）。通过预训练的神经检索器，这种组合被赋予了新的生命。RAG 模型的目标是利用这两种记忆类型，使语言生成更加生动和具有创造力。</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"3\">\n<li><strong>两种RAG形式</strong>：研究精心比较了两种 RAG 形式。一种在整个生成过程中使用相同的检索段落，另一种则可以为每个词汇使用不同的段落。。</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"4\">\n<li><strong>性能与评估</strong>：在多种知识密集型 NLP 任务中，RAG 模型经过微调和评估后，创造了三个开放领域问答任务的新纪录，超越了传统的参数型 seq2seq 模型和专门的检索-提取架构。在语言生成方面，与仅使用参数的 seq2seq 模型相比，RAG 模型能生成更加具体、多样和真实的语言。</li>\n</ol>\n</blockquote>\n<p>整体看一下：论文对RAG技术的理解。 主要由 <strong>Retriever</strong>和<strong>Generator</strong>两大部分组成。<br><img src=\"https://s.zhangguiyi.cn/vent/202312181613600.png\" alt=\"image.png\"></p>\n<p>随着时间的推移，尤其是大模型的进步，RAG的架构有些变化，更加组件化和清晰化。<br>如果希望了解更多关于RAG的历史和发展，推荐仔细阅读综述文章： <strong><a href=\"https://arxiv.org/pdf/2312.10997.pdf\">Retrieval-Augmented Generation for Large Language Models: A Survey”</a></strong> </p>\n<p><img src=\"https://s.alidraft.com/vent/202312211120837.png\" alt=\"image.png\"></p>\n<h3 id=\"2-进一步理解RAG\"><a href=\"#2-进一步理解RAG\" class=\"headerlink\" title=\"2. 进一步理解RAG\"></a>2. 进一步理解RAG</h3><p>第一节相对学术化一些，我们来看一个更好理解的图：</p>\n<p><img src=\"https://docs.llamaindex.ai/en/stable/_images/basic_rag.png\"></p>\n<p>从上图可以看到<strong>RAG</strong>的系统核心,由User(Query),(Vector) Index , LLM 三大组件组成。</p>\n<h4 id=\"2-1-三大组件\"><a href=\"#2-1-三大组件\" class=\"headerlink\" title=\"2.1 三大组件\"></a>2.1 三大组件</h4><ol>\n<li>**用户发起的查询(User-&gt;query)**。这种查询一般都是自然语言的，用户不再需要学习类似于之前搜索引擎的DSL或者数据库的SQL。这样大幅降低这类系统的使用门槛。</li>\n<li><strong>模型所需的外部数据(Index -&gt; prompt)<strong>。可以看到，RAG系统的核心工作其实在这个组件。<br> 1. <strong>索引</strong>：将不同类似的用户数据，比如结构化的关系数据库、非结构化的文本、甚至是可编程的API，通过向量嵌入(Vector embedding)方法来将它们变成向量数据。更多关于向量数据库的文章，可以访问我之前的博客。涉及到</strong>向量化(Vector embedding)方法</strong>和<strong>切片(Chunk)方法</strong>。<br> 2. <strong>召回</strong>，向量数据的召回，本质上通过余弦相似度来找到最匹配的多个向量。目的是从大量数据中快速筛选出与查询最相关的文档子集，为后续的更详细检索过程提供一个更专注的候选集。这种方法旨在提高检索过程的效率和效果，减少计算资源的需求，并加速响应时间。<br> 3. <strong>查询</strong>。这里查询方法有很多种，涉及到向量数据库的相关度计算与评估，不同的查询策略。</li>\n<li>**大模型(LLM)**。这里面的大模型可以是开源的Llama2&#x2F;Mistral 等，也可以是闭源的GPT3.5&#x2F;4等。</li>\n</ol>\n<h4 id=\"2-2-实现RAG的五个步骤\"><a href=\"#2-2-实现RAG的五个步骤\" class=\"headerlink\" title=\"2.2 实现RAG的五个步骤\"></a>2.2 实现RAG的五个步骤</h4><p>重复总结一下，实现RAG中有五个关键步骤，如下图所示：<br><img src=\"https://docs.llamaindex.ai/en/stable/_images/stages.png\"></p>\n<ul>\n<li><strong>加载</strong>：指将各种文本文件、PDF、其他网站、数据库还是API等数据，导入我们工作流的步骤。</li>\n<li><strong>索引</strong>：和普通关系数据库无本质差异，在于通过索引加速查询。不同的是，具体的索引算法。</li>\n<li><strong>存储</strong>：存储索引以及其他元数据，以避免重新索引</li>\n<li><strong>查询</strong>：对于任何给定的索引，可以进行多种查询，包括子查询、多步查询和混合策略。</li>\n<li><strong>评估</strong>：提供了关于查询的响应有多准确、快速的客观衡量。</li>\n</ul>\n<h3 id=\"3-RAG的进化\"><a href=\"#3-RAG的进化\" class=\"headerlink\" title=\"3. RAG的进化\"></a>3. RAG的进化</h3><p><img src=\"https://s.alidraft.com/vent/202312211123188.png\" alt=\"image.png\"></p>\n<ol>\n<li><p><strong>初级RAG（Naive RAG）</strong>：这是RAG的最初形式，包括基本的索引、检索和生成过程。它以简单的方式将检索到的信息与生成任务相结合，但可能存在准确性和效率的问题。</p>\n</li>\n<li><p><strong>高级RAG（Advanced RAG）</strong>：在初级RAG的基础上，高级RAG引入了预检索和后检索处理方法，优化了索引和检索流程。这种范式致力于提高检索内容的质量和相关性，以及提升生成任务的效果。</p>\n</li>\n<li><p><strong>模块化RAG（Modular RAG）</strong>：这种范式通过引入多样的模块，如搜索模块、记忆模块和额外的生成模块，提供了更多的灵活性和定制化能力。模块化RAG允许根据特定问题的上下文重新配置或替换模块，实现更复杂和高效的检索增强生成任务。</p>\n</li>\n</ol>\n<p>再次推荐阅读综述文章： <strong><a href=\"https://arxiv.org/pdf/2312.10997.pdf\">Retrieval-Augmented Generation for Large Language Models: A Survey”</a></strong> </p>\n<h3 id=\"4-RAG能干嘛？\"><a href=\"#4-RAG能干嘛？\" class=\"headerlink\" title=\"4. RAG能干嘛？\"></a>4. RAG能干嘛？</h3><p>RAG的LLM应用的用例无穷无尽，但大致可以分为三类：</p>\n<p><a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html\"><strong>查询引擎</strong></a>: 查询引擎允许您对您的数据提出问题。它接收自然语言查询，并返回响应，以及检索并传递给LLM的参考上下文。</p>\n<p><a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html\"><strong>聊天引擎</strong></a>： 聊天引擎用于与您的数据进行<strong>多轮对话</strong>。</p>\n<p><a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html\"><strong>Agent(代理)</strong></a>: Agent由LLM驱动，能够实现自动决策。可以采取任意数量的步骤来完成给定的任务，动态地决定最佳行动方案。Agent某种意义上来讲是一种AGI。</p>\n<h3 id=\"5-给我也搞一个\"><a href=\"#5-给我也搞一个\" class=\"headerlink\" title=\"5. 给我也搞一个\"></a>5. 给我也搞一个</h3><p><strong>可以！</strong><br>接下来我们基于<a href=\"https://www.llamaindex.ai/\">Llama Index</a>库来实现一个网页数据的Q&amp;A机器人。</p>\n<h2 id=\"RAG实战\"><a href=\"#RAG实战\" class=\"headerlink\" title=\"RAG实战\"></a>RAG实战</h2><p>2023 年以来，出现了大量的开源 &amp; 闭源LLM大模型，基本上都能够在上面构建 RAG 系统。<br>最常见的方式包括： </p>\n<ul>\n<li>GPT-3.5&#x2F;4 + RAG（闭源方案） </li>\n<li>Llama 2 &#x2F; Mistral + RAG（开源方案）</li>\n</ul>\n<h3 id=\"基于LLama-Index-和-GPT3-5-来构建\"><a href=\"#基于LLama-Index-和-GPT3-5-来构建\" class=\"headerlink\" title=\"基于LLama-Index 和 GPT3.5 来构建\"></a>基于LLama-Index 和 GPT3.5 来构建</h3><p>我们基于来LLama-Index 和 GPT3.5 来构建一个RAG系统，它能够访问你指定的<strong>网页数据</strong>，你可以提问关于这个网页的<strong>任何内容</strong>。</p>\n<blockquote>\n<ol>\n<li>Llama-Index是一个简单灵活的数据框架，用于连接自定义数据源到大型语言模型（LLM）。</li>\n<li>它提供了API和入门教程，方便用户进行数据的摄取和查询。</li>\n<li>Llama-Index可以作为桥梁，连接自定义数据和大型语言模型。</li>\n<li>通过Llama-Index，用户可以轻松构建应用程序，并访问私有或特定领域的数据。</li>\n</ol>\n</blockquote>\n<ul>\n<li>复习一遍流程：加载、索引、存储、查询、评估</li>\n</ul>\n<h3 id=\"加载库和数据\"><a href=\"#加载库和数据\" class=\"headerlink\" title=\"加载库和数据\"></a>加载库和数据</h3><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 安装所需的库</span></span><br><span class=\"line\">!pip install llama-index transformers</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> llama_index.readers <span class=\"keyword\">import</span> BeautifulSoupWebReader</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># 访问智写AI的官网博客</span></span><br><span class=\"line\">url = <span class=\"string\">&quot;https://www.draftai.cn/2023/12/19/chatonce-support-chat-with-file/&quot;</span></span><br><span class=\"line\"><span class=\"comment\"># 通过BeautifulSoupWebReader 来加载数据</span></span><br><span class=\"line\">documents = BeautifulSoupWebReader().load_data([url])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"索引-存储\"><a href=\"#索引-存储\" class=\"headerlink\" title=\"索引&amp;存储\"></a>索引&amp;存储</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## index </span></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> openai</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#设置你在openai的密钥</span></span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;OPENAI_API_KEY&#x27;</span>] = <span class=\"string\">&quot;sk-&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">openai.api_key = os.environ[<span class=\"string\">&#x27;OPENAI_API_KEY&#x27;</span>]</span><br><span class=\"line\"><span class=\"keyword\">from</span> llama_index.llms <span class=\"keyword\">import</span> OpenAI</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\">## 指定GPT3.5模型，记得要用gpt-3.5-turbo-1106，更便宜</span></span><br><span class=\"line\">llm = OpenAI(model=<span class=\"string\">&quot;gpt-3.5-turbo-1106&quot;</span>, temperature=<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"keyword\">from</span> llama_index <span class=\"keyword\">import</span> ServiceContext</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\">## 向量化，采用BAAI的向量库，开源免费，比用OpenAI的embbeding便宜。</span></span><br><span class=\"line\">service_context = ServiceContext.from_defaults(llm=llm, embed_model=<span class=\"string\">&quot;local:BAAI/bge-small-zh-v1.5&quot;</span>) <span class=\"comment\">#BAAI/bge-small-zh-v1.5. BAAI/bge-small-en-v1.5</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"查询\"><a href=\"#查询\" class=\"headerlink\" title=\"查询\"></a>查询</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> llama_index.response.notebook_utils <span class=\"keyword\">import</span> display_response</span><br><span class=\"line\"><span class=\"keyword\">import</span> logging</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># 打印日志组件</span></span><br><span class=\"line\">logging.basicConfig(stream=sys.stdout, level=logging.INFO)</span><br><span class=\"line\"></span><br><span class=\"line\">logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))</span><br><span class=\"line\">query_engine = vector_index.as_query_engine(response_mode=<span class=\"string\">&quot;compact&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># 简单查询问题</span></span><br><span class=\"line\">response = query_engine.query(<span class=\"string\">&quot;智写AI能干嘛?它最新的功能是什么？&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># 展示返回结果</span></span><br><span class=\"line\">display_response(response)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"不同的查询策略和效果\"><a href=\"#不同的查询策略和效果\" class=\"headerlink\" title=\"不同的查询策略和效果\"></a>不同的查询策略和效果</h3><p><img src=\"https://s.alidraft.com/vent/202312211901720.png\" alt=\"image.png\"></p>\n<p><img src=\"https://s.alidraft.com/vent/202312211900917.png\" alt=\"image.png\"></p>\n<p><img src=\"https://s.alidraft.com/vent/202312211903829.png\" alt=\"image.png\"></p>\n<h3 id=\"完整代码\"><a href=\"#完整代码\" class=\"headerlink\" title=\"完整代码\"></a>完整代码</h3><p>可以复制我在Colab的<a href=\"https://colab.research.google.com/drive/1gvqOOpxduIKS3EPCwG3mkn8hnEmWED1L?usp=sharing\">notebook </a>直接运行。</p>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p>[1]Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks: <em><a href=\"https://arxiv.org/abs/2005.11401\">https://arxiv.org/abs/2005.11401</a></em></p>\n<p>[2]<strong>查询引擎</strong>: <em><a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html\">https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html</a></em></p>\n<p>[3]<strong>聊天引擎</strong>: <em><a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html\">https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html</a></em></p>\n<p>[4]<strong>Agent(代理)</strong>: <em><a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html\">https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html</a></em></p>\n<p>[5]Llama Index: <em><a href=\"https://www.llamaindex.ai/\">https://www.llamaindex.ai/</a></em></p>\n<p>[6]notebook : <em><a href=\"https://colab.research.google.com/drive/1gvqOOpxduIKS3EPCwG3mkn8hnEmWED1L?usp=sharing\">https://colab.research.google.com/drive/1gvqOOpxduIKS3EPCwG3mkn8hnEmWED1L?usp=sharing</a></em></p>\n","excerpt":"","more":"<p><img src=\"https://s.alidraft.com/vent/vent1924_convey_a_sense_of_cutting-edge_technology_and_innovati_58510eb0-17a4-4f8a-baa3-4c159386c609.png\" alt=\"vent1924_convey_a_sense_of_cutting-edge_technology_and_innovati_58510eb0-17a4-4f8a-baa3-4c159386c609.png\"></p>\n<h2 id=\"RAG理论\"><a href=\"#RAG理论\" class=\"headerlink\" title=\"RAG理论\"></a>RAG理论</h2><h3 id=\"1-什么是RAG\"><a href=\"#1-什么是RAG\" class=\"headerlink\" title=\"1. 什么是RAG\"></a>1. 什么是RAG</h3><p>众所周知，大模型基于海量的数据来训练，它具备非常强大的智能，能够回答各种问题。但是我们在使用过程中发现，通用大模型在某些专业领域能力还不够强，很多<strong>领域相关问题很难回答得上来</strong>。通常，预训练（pre-train）的大模型只懂得它训练时用的数据，对于训练集之外的新信息（比如网络搜索新数据或特定行业的知识）就不太清楚。</p>\n<p>那么怎么构建一个私有的GPT大模型呢？方法有很多种，包括 1. 重新训练<strong>私有领域数据</strong>的大模型，2. 基于已有大模型做专有数据的<strong>微调</strong>(FineTuning) 3. 通过RAG技术，优化大模型基础能力。4. 通过Prompt 工程把私有数据在对话中给到大模型。</p>\n<p><strong>RAG</strong>: Retrieval Augmented Generation，检索增强生成技术。RAG由FaceBook AI实验室 于2020年提出，他们的论文<a href=\"https://arxiv.org/abs/2005.11401\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a>， 提供了一种通过给大模型提供向量数据来增强模型能力的方法。</p>\n<p>快速来看一下文章摘要：</p>\n<blockquote>\n<ol>\n<li><strong>背景与挑战</strong>：这些大型预训练语言模型，因其存储了大量事实知识和在 NLP 任务中的出色表现而闻名。但它们在精确获取和处理知识方面存在局限，尤其是在知识要求高的任务中。这导致它们在特定任务的架构上表现不佳。此外，如何提供决策依据和更新模型中的知识仍是一个挑战。</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"2\">\n<li><strong>检索增强生成（RAG）方法</strong>：文章提出了一种 RAG 模型的微调方案，这些模型巧妙地结合了预训练的参数记忆（例如 seq2seq 模型）和非参数记忆（例如维基百科的密集向量索引）。通过预训练的神经检索器，这种组合被赋予了新的生命。RAG 模型的目标是利用这两种记忆类型，使语言生成更加生动和具有创造力。</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"3\">\n<li><strong>两种RAG形式</strong>：研究精心比较了两种 RAG 形式。一种在整个生成过程中使用相同的检索段落，另一种则可以为每个词汇使用不同的段落。。</li>\n</ol>\n</blockquote>\n<blockquote>\n<ol start=\"4\">\n<li><strong>性能与评估</strong>：在多种知识密集型 NLP 任务中，RAG 模型经过微调和评估后，创造了三个开放领域问答任务的新纪录，超越了传统的参数型 seq2seq 模型和专门的检索-提取架构。在语言生成方面，与仅使用参数的 seq2seq 模型相比，RAG 模型能生成更加具体、多样和真实的语言。</li>\n</ol>\n</blockquote>\n<p>整体看一下：论文对RAG技术的理解。 主要由 <strong>Retriever</strong>和<strong>Generator</strong>两大部分组成。<br><img src=\"https://s.zhangguiyi.cn/vent/202312181613600.png\" alt=\"image.png\"></p>\n<p>随着时间的推移，尤其是大模型的进步，RAG的架构有些变化，更加组件化和清晰化。<br>如果希望了解更多关于RAG的历史和发展，推荐仔细阅读综述文章： <strong><a href=\"https://arxiv.org/pdf/2312.10997.pdf\">Retrieval-Augmented Generation for Large Language Models: A Survey”</a></strong> </p>\n<p><img src=\"https://s.alidraft.com/vent/202312211120837.png\" alt=\"image.png\"></p>\n<h3 id=\"2-进一步理解RAG\"><a href=\"#2-进一步理解RAG\" class=\"headerlink\" title=\"2. 进一步理解RAG\"></a>2. 进一步理解RAG</h3><p>第一节相对学术化一些，我们来看一个更好理解的图：</p>\n<p><img src=\"https://docs.llamaindex.ai/en/stable/_images/basic_rag.png\"></p>\n<p>从上图可以看到<strong>RAG</strong>的系统核心,由User(Query),(Vector) Index , LLM 三大组件组成。</p>\n<h4 id=\"2-1-三大组件\"><a href=\"#2-1-三大组件\" class=\"headerlink\" title=\"2.1 三大组件\"></a>2.1 三大组件</h4><ol>\n<li>**用户发起的查询(User-&gt;query)**。这种查询一般都是自然语言的，用户不再需要学习类似于之前搜索引擎的DSL或者数据库的SQL。这样大幅降低这类系统的使用门槛。</li>\n<li><strong>模型所需的外部数据(Index -&gt; prompt)<strong>。可以看到，RAG系统的核心工作其实在这个组件。<br> 1. <strong>索引</strong>：将不同类似的用户数据，比如结构化的关系数据库、非结构化的文本、甚至是可编程的API，通过向量嵌入(Vector embedding)方法来将它们变成向量数据。更多关于向量数据库的文章，可以访问我之前的博客。涉及到</strong>向量化(Vector embedding)方法</strong>和<strong>切片(Chunk)方法</strong>。<br> 2. <strong>召回</strong>，向量数据的召回，本质上通过余弦相似度来找到最匹配的多个向量。目的是从大量数据中快速筛选出与查询最相关的文档子集，为后续的更详细检索过程提供一个更专注的候选集。这种方法旨在提高检索过程的效率和效果，减少计算资源的需求，并加速响应时间。<br> 3. <strong>查询</strong>。这里查询方法有很多种，涉及到向量数据库的相关度计算与评估，不同的查询策略。</li>\n<li>**大模型(LLM)**。这里面的大模型可以是开源的Llama2&#x2F;Mistral 等，也可以是闭源的GPT3.5&#x2F;4等。</li>\n</ol>\n<h4 id=\"2-2-实现RAG的五个步骤\"><a href=\"#2-2-实现RAG的五个步骤\" class=\"headerlink\" title=\"2.2 实现RAG的五个步骤\"></a>2.2 实现RAG的五个步骤</h4><p>重复总结一下，实现RAG中有五个关键步骤，如下图所示：<br><img src=\"https://docs.llamaindex.ai/en/stable/_images/stages.png\"></p>\n<ul>\n<li><strong>加载</strong>：指将各种文本文件、PDF、其他网站、数据库还是API等数据，导入我们工作流的步骤。</li>\n<li><strong>索引</strong>：和普通关系数据库无本质差异，在于通过索引加速查询。不同的是，具体的索引算法。</li>\n<li><strong>存储</strong>：存储索引以及其他元数据，以避免重新索引</li>\n<li><strong>查询</strong>：对于任何给定的索引，可以进行多种查询，包括子查询、多步查询和混合策略。</li>\n<li><strong>评估</strong>：提供了关于查询的响应有多准确、快速的客观衡量。</li>\n</ul>\n<h3 id=\"3-RAG的进化\"><a href=\"#3-RAG的进化\" class=\"headerlink\" title=\"3. RAG的进化\"></a>3. RAG的进化</h3><p><img src=\"https://s.alidraft.com/vent/202312211123188.png\" alt=\"image.png\"></p>\n<ol>\n<li><p><strong>初级RAG（Naive RAG）</strong>：这是RAG的最初形式，包括基本的索引、检索和生成过程。它以简单的方式将检索到的信息与生成任务相结合，但可能存在准确性和效率的问题。</p>\n</li>\n<li><p><strong>高级RAG（Advanced RAG）</strong>：在初级RAG的基础上，高级RAG引入了预检索和后检索处理方法，优化了索引和检索流程。这种范式致力于提高检索内容的质量和相关性，以及提升生成任务的效果。</p>\n</li>\n<li><p><strong>模块化RAG（Modular RAG）</strong>：这种范式通过引入多样的模块，如搜索模块、记忆模块和额外的生成模块，提供了更多的灵活性和定制化能力。模块化RAG允许根据特定问题的上下文重新配置或替换模块，实现更复杂和高效的检索增强生成任务。</p>\n</li>\n</ol>\n<p>再次推荐阅读综述文章： <strong><a href=\"https://arxiv.org/pdf/2312.10997.pdf\">Retrieval-Augmented Generation for Large Language Models: A Survey”</a></strong> </p>\n<h3 id=\"4-RAG能干嘛？\"><a href=\"#4-RAG能干嘛？\" class=\"headerlink\" title=\"4. RAG能干嘛？\"></a>4. RAG能干嘛？</h3><p>RAG的LLM应用的用例无穷无尽，但大致可以分为三类：</p>\n<p><a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html\"><strong>查询引擎</strong></a>: 查询引擎允许您对您的数据提出问题。它接收自然语言查询，并返回响应，以及检索并传递给LLM的参考上下文。</p>\n<p><a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html\"><strong>聊天引擎</strong></a>： 聊天引擎用于与您的数据进行<strong>多轮对话</strong>。</p>\n<p><a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html\"><strong>Agent(代理)</strong></a>: Agent由LLM驱动，能够实现自动决策。可以采取任意数量的步骤来完成给定的任务，动态地决定最佳行动方案。Agent某种意义上来讲是一种AGI。</p>\n<h3 id=\"5-给我也搞一个\"><a href=\"#5-给我也搞一个\" class=\"headerlink\" title=\"5. 给我也搞一个\"></a>5. 给我也搞一个</h3><p><strong>可以！</strong><br>接下来我们基于<a href=\"https://www.llamaindex.ai/\">Llama Index</a>库来实现一个网页数据的Q&amp;A机器人。</p>\n<h2 id=\"RAG实战\"><a href=\"#RAG实战\" class=\"headerlink\" title=\"RAG实战\"></a>RAG实战</h2><p>2023 年以来，出现了大量的开源 &amp; 闭源LLM大模型，基本上都能够在上面构建 RAG 系统。<br>最常见的方式包括： </p>\n<ul>\n<li>GPT-3.5&#x2F;4 + RAG（闭源方案） </li>\n<li>Llama 2 &#x2F; Mistral + RAG（开源方案）</li>\n</ul>\n<h3 id=\"基于LLama-Index-和-GPT3-5-来构建\"><a href=\"#基于LLama-Index-和-GPT3-5-来构建\" class=\"headerlink\" title=\"基于LLama-Index 和 GPT3.5 来构建\"></a>基于LLama-Index 和 GPT3.5 来构建</h3><p>我们基于来LLama-Index 和 GPT3.5 来构建一个RAG系统，它能够访问你指定的<strong>网页数据</strong>，你可以提问关于这个网页的<strong>任何内容</strong>。</p>\n<blockquote>\n<ol>\n<li>Llama-Index是一个简单灵活的数据框架，用于连接自定义数据源到大型语言模型（LLM）。</li>\n<li>它提供了API和入门教程，方便用户进行数据的摄取和查询。</li>\n<li>Llama-Index可以作为桥梁，连接自定义数据和大型语言模型。</li>\n<li>通过Llama-Index，用户可以轻松构建应用程序，并访问私有或特定领域的数据。</li>\n</ol>\n</blockquote>\n<ul>\n<li>复习一遍流程：加载、索引、存储、查询、评估</li>\n</ul>\n<h3 id=\"加载库和数据\"><a href=\"#加载库和数据\" class=\"headerlink\" title=\"加载库和数据\"></a>加载库和数据</h3><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 安装所需的库</span></span><br><span class=\"line\">!pip install llama-index transformers</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> llama_index.readers <span class=\"keyword\">import</span> BeautifulSoupWebReader</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># 访问智写AI的官网博客</span></span><br><span class=\"line\">url = <span class=\"string\">&quot;https://www.draftai.cn/2023/12/19/chatonce-support-chat-with-file/&quot;</span></span><br><span class=\"line\"><span class=\"comment\"># 通过BeautifulSoupWebReader 来加载数据</span></span><br><span class=\"line\">documents = BeautifulSoupWebReader().load_data([url])</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"索引-存储\"><a href=\"#索引-存储\" class=\"headerlink\" title=\"索引&amp;存储\"></a>索引&amp;存储</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## index </span></span><br><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> openai</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#设置你在openai的密钥</span></span><br><span class=\"line\">os.environ[<span class=\"string\">&#x27;OPENAI_API_KEY&#x27;</span>] = <span class=\"string\">&quot;sk-&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">openai.api_key = os.environ[<span class=\"string\">&#x27;OPENAI_API_KEY&#x27;</span>]</span><br><span class=\"line\"><span class=\"keyword\">from</span> llama_index.llms <span class=\"keyword\">import</span> OpenAI</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\">## 指定GPT3.5模型，记得要用gpt-3.5-turbo-1106，更便宜</span></span><br><span class=\"line\">llm = OpenAI(model=<span class=\"string\">&quot;gpt-3.5-turbo-1106&quot;</span>, temperature=<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"keyword\">from</span> llama_index <span class=\"keyword\">import</span> ServiceContext</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\">## 向量化，采用BAAI的向量库，开源免费，比用OpenAI的embbeding便宜。</span></span><br><span class=\"line\">service_context = ServiceContext.from_defaults(llm=llm, embed_model=<span class=\"string\">&quot;local:BAAI/bge-small-zh-v1.5&quot;</span>) <span class=\"comment\">#BAAI/bge-small-zh-v1.5. BAAI/bge-small-en-v1.5</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h3 id=\"查询\"><a href=\"#查询\" class=\"headerlink\" title=\"查询\"></a>查询</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> llama_index.response.notebook_utils <span class=\"keyword\">import</span> display_response</span><br><span class=\"line\"><span class=\"keyword\">import</span> logging</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> sys</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># 打印日志组件</span></span><br><span class=\"line\">logging.basicConfig(stream=sys.stdout, level=logging.INFO)</span><br><span class=\"line\"></span><br><span class=\"line\">logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))</span><br><span class=\"line\">query_engine = vector_index.as_query_engine(response_mode=<span class=\"string\">&quot;compact&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># 简单查询问题</span></span><br><span class=\"line\">response = query_engine.query(<span class=\"string\">&quot;智写AI能干嘛?它最新的功能是什么？&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"comment\"># 展示返回结果</span></span><br><span class=\"line\">display_response(response)</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"不同的查询策略和效果\"><a href=\"#不同的查询策略和效果\" class=\"headerlink\" title=\"不同的查询策略和效果\"></a>不同的查询策略和效果</h3><p><img src=\"https://s.alidraft.com/vent/202312211901720.png\" alt=\"image.png\"></p>\n<p><img src=\"https://s.alidraft.com/vent/202312211900917.png\" alt=\"image.png\"></p>\n<p><img src=\"https://s.alidraft.com/vent/202312211903829.png\" alt=\"image.png\"></p>\n<h3 id=\"完整代码\"><a href=\"#完整代码\" class=\"headerlink\" title=\"完整代码\"></a>完整代码</h3><p>可以复制我在Colab的<a href=\"https://colab.research.google.com/drive/1gvqOOpxduIKS3EPCwG3mkn8hnEmWED1L?usp=sharing\">notebook </a>直接运行。</p>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p>[1]Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks: <em><a href=\"https://arxiv.org/abs/2005.11401\">https://arxiv.org/abs/2005.11401</a></em></p>\n<p>[2]<strong>查询引擎</strong>: <em><a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html\">https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/root.html</a></em></p>\n<p>[3]<strong>聊天引擎</strong>: <em><a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html\">https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/root.html</a></em></p>\n<p>[4]<strong>Agent(代理)</strong>: <em><a href=\"https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html\">https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/root.html</a></em></p>\n<p>[5]Llama Index: <em><a href=\"https://www.llamaindex.ai/\">https://www.llamaindex.ai/</a></em></p>\n<p>[6]notebook : <em><a href=\"https://colab.research.google.com/drive/1gvqOOpxduIKS3EPCwG3mkn8hnEmWED1L?usp=sharing\">https://colab.research.google.com/drive/1gvqOOpxduIKS3EPCwG3mkn8hnEmWED1L?usp=sharing</a></em></p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"clqjhnr950000cu9p2dxx94fx","category_id":"clqjhnr990001cu9phgy21k6e","_id":"clqjhnr9b0004cu9pcpyn19yg"}],"PostTag":[{"post_id":"clqjhnr950000cu9p2dxx94fx","tag_id":"clqjhnr9a0002cu9pfo3i6on8","_id":"clqjhnr9c0007cu9p8fmvb8qc"},{"post_id":"clqjhnr950000cu9p2dxx94fx","tag_id":"clqjhnr9a0003cu9p052i1vau","_id":"clqjhnr9c0008cu9pe67s7amv"},{"post_id":"clqjhnr950000cu9p2dxx94fx","tag_id":"clqjhnr9b0005cu9p0w270qnb","_id":"clqjhnr9c0009cu9p0vi568ru"},{"post_id":"clqjhnr950000cu9p2dxx94fx","tag_id":"clqjhnr9b0006cu9p5mjbbobc","_id":"clqjhnr9c000acu9pewv9dkux"}],"Tag":[{"name":"vector","_id":"clqjhnr9a0002cu9pfo3i6on8"},{"name":"LLM","_id":"clqjhnr9a0003cu9p052i1vau"},{"name":"RAG","_id":"clqjhnr9b0005cu9p0w270qnb"},{"name":"embeeding","_id":"clqjhnr9b0006cu9p5mjbbobc"}]}}